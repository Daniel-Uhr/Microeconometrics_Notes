{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortogonalização, Double Machine Learning e CATE\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* Ortogonalização\n",
    "  * Aplicação do Procedimento de Ortogonalização no Python\n",
    "* DML - Orthogonal/Double Machine Learning\n",
    "  * Partially Linear Regression Models (PLR)\n",
    "  * Aplicações do DML-Manual para identificar o ATE\n",
    "  * Aplicações do DML atraés de pacotes para identificar o ATE\n",
    "  * Outros Modelos de DML para identificar o ATE\n",
    "    * Partially Linear IV Regression Models (PLIV)\n",
    "    * Interactive regression models (IRM)\n",
    "    * Interactive IV regression models (IIVM)\n",
    "* Arcabouço de Resultados Potenciais, Individual Treatment Effects (ITE) e Conditional Average Treatment Effects (CATE)\n",
    "  * Estimação do CATE com DML\n",
    "* Estimação do CATE com DML no Python\n",
    "\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais:**\n",
    "* Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, Volume 21, Issue 1, 1 February 2018, Pages C1–C68, https://doi.org/10.1111/ectj.12097\n",
    "  \n",
    "* Microsoft EconML: https://econml.azurewebsites.net/\n",
    "* UBER CausalML: https://causalml.readthedocs.io/en/latest/\n",
    "* DoubleML for python: https://github.com/DoubleML/doubleml-for-py ou https://docs.doubleml.org/stable/index.html\n",
    "\n",
    "**Complementares:**\n",
    "* Chernozhukov, V. and C. Hansen (2004). The effects of 401 (k) participation on the wealth distribution: an instrumental quantile regression analysis. Review of Economics and Statistics 86, 735–51. \n",
    "* Chernozhukov, V., D. Chetverikov and K. Kato (2014). Gaussian approximation of suprema of empirical processes. Annals of Statistics 42, 1564–97. \n",
    "* Chernozhukov, V., J. Escanciano, H. Ichimura, W. Newey and J. Robins (2016). Locally robust semiparametric estimation. Preprint (arXiv:1608.00033). \n",
    "* Chernozhukov, V., C. Hansen and M. Spindler (2015a). Post-selection and post-regularization inference in linear models with very many controls and instruments. Americal Economic Review: Papers and Proceedings 105, 486–90. \n",
    "* Chernozhukov, V., C. Hansen and M. Spindler (2015b). Valid post-selection and post-regularization inference: an elementary, general approach. Annual Review of Economics 7, 649–88.\n",
    "* Bach, P., Chernozhukov, V., Kurz, M. S., and Spindler, M. (2022), DoubleML - An Object-Oriented Implementation of Double Machine Learning in Python, Journal of Machine Learning Research, 23(53): 1-6, https://www.jmlr.org/papers/v23/21-0862.html.\n",
    "* Bach, P., Chernozhukov, V., Kurz, M. S., Spindler, M. and Klaassen, S. (2024), DoubleML - An Object-Oriented Implementation of Double Machine Learning in R, Journal of Statistical Software, 108(3): 1-56, doi:10.18637/jss.v108.i03, arXiv:2103.09603.\n",
    "* Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W. and Robins, J. (2018), Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21: C1-C68, doi:10.1111/ectj.12097.\n",
    "* Lang, M., Binder, M., Richter, J., Schratz, P., Pfisterer, F., Coors, S., Au, Q., Casalicchio, G., Kotthoff, L. and Bischl, B. (2019), mlr3: A modern object-oriented machine learing framework in R. Journal of Open Source Software, doi:10.21105/joss.01903.\n",
    "* Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M. and Duchesnay, E. (2011), Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12: 2825–2830, https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortogonalização\n",
    "\n",
    "A ideia de ortogonalização é baseada em um teorema elaborado por três econometristas em 1933, Ragnar Frisch, Frederick V. Waugh e Michael C. Lovell. Simplificando, afirma que você pode decompor qualquer modelo de regressão linear multivariável em três estágios ou modelos. \n",
    "\n",
    "Digamos que você tem uma matriz de covariáveis $X$, e voce particiona ela em duas partes, $X_{1}$ e $D$. \n",
    "\n",
    "* **Primeira Etapa**\n",
    "\n",
    "Pegamos o primeiro conjunto de variáveis $X_{1}$ e fazemos uma regressão linear de $X_{1}$ em $Y$, onde $\\theta_{1}$ é o vetor de parâmetros\n",
    "\n",
    "$$ y_{i} = \\theta_{0} + \\theta_{1} X_{1i} + \\epsilon_{i}$$\n",
    "\n",
    "e guardamos os resíduos dessa regressão ($y^{*}$).\n",
    "\n",
    "$$ y^{*}_{i} = y_{i} - \\hat{y}_{i} = y_{i} - ( \\hat{\\theta}_{0} + \\hat{\\theta}_{1} X_{1i} )$$\n",
    "\n",
    "* **Segunda Etapa**\n",
    "\n",
    "Pegamos novamente o primeiro conjunto de características, mas agora executamos um modelo onde estimamos o segundo conjunto de características ($D$)\n",
    "\n",
    "$$ D_{i} = \\gamma_{0} + \\gamma_{1} X_{1i} + e_{i}$$\n",
    "\n",
    "Aqui, estamos usando o primeiro conjunto de recursos para prever o segundo conjunto de recursos. Por fim, consideramos também os resíduos desta segunda etapa.\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - (\\hat{\\gamma_{0}} + \\hat{\\gamma_{1}} X_{1i})$$\n",
    "\n",
    "* **Terceira etapa**\n",
    "\n",
    "Por fim, pegamos os resíduos do primeiro e do segundo estágio e estimamos o seguinte modelo\n",
    "\n",
    "$$ y_{i}^{*} = \\alpha_{0} + \\beta_{2} D_{i}^{*} + e_{i}$$\n",
    "\n",
    "\n",
    "* **Teorema Frisch – Waugh – Lovell (FWL)**\n",
    "\n",
    "O teorema FWL afirma que a estimativa do parâmetro $\\hat{\\beta}_{2}$ (ATE), estimado anteriormente, é equivalente ao que obtemos ao executar a regressão completa, com todas as covariáveis.\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1} X_{1i} + \\beta_{2} D_{i} + e_{i}$$\n",
    "\n",
    "\n",
    "**Intuição do teorema FWL**\n",
    "\n",
    "Sabemos que a regressão é um modelo muito especial. Cada um de seus parâmetros tem a interpretação de uma derivada parcial, quanto seria Y se X aumentasse em uma unidade, mantendo todos as outras covariáveis constantes. Sabemos também que se omitirmos variáveis ​​da regressão, teremos viés. Especificamente, viés variável omitido (ou viés de confusão). Ainda assim, Frisch-Waugh-Lovell está dizendo que posso dividir meu modelo de regressão em duas partes, nenhuma delas contendo o conjunto completo de recursos, e ainda assim obter a mesma estimativa que obteria executando a regressão inteira. \n",
    "\n",
    "O teorema fornece algumas dicas sobre o que a regressão linear está fazendo. Para obter o coeficiente de uma variável $X_{k}$, a regressão primeiro usa todas as outras variáveis ​​para prever $X_{k}$ e pega os resíduos. Isso “limpa” $X_{k}$ de qualquer influência dessas variáveis. Dessa forma, quando tentamos entender o impacto de $X_{k}$ sobre $Y$, estará livre de viés de variável omitida. Em segundo lugar, a regressão usa todas as outras variáveis ​​para prever $Y$ e pega os resíduos. Isso “limpa” $Y$ de qualquer influência dessas variáveis, reduzindo a variância de $Y$ para que seja mais fácil ver como $X_{k}$ afeta $Y$.\n",
    "\n",
    "A regressão linear está estimando o impacto de $D$ sobre $y$ enquanto contabiliza $X_{1}$. Isso é importante para inferência causal. \n",
    "\n",
    "Assim, podemos construir um modelo que preveja um tratamento ($D$) usando as covariáveis $X$, um modelo que prevê o resultado $y$ usando as mesmas covariáveis, pegar os resíduos de ambos os modelos e executar um modelo que estime como o resíduo de $D$ afetam os resíduos de $y$. Este último modelo vai me dizer como $D$ afeta $y$ enquanto controla por $X$. \n",
    "\n",
    "Ou seja, **os dois primeiros modelos controlam as variáveis de confusão**. Eles estão **gerando dados que são praticamente aleatórios**. Lembre que é isso que estaria distorcendo seus dados. Então, usamos no modelo final para estimar o efeito causal de interesse **Average Treatment Effect - ATE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do Procedimento de Ortogonalização no Python\n",
    "\n",
    "Vamos aplicar o procedimento de ortogonalização considerando um modelo de regressão linear simples. Vamos realizar a orgonalização supondo linearidade entre as variáveis para entender o conceito. Posteriormente, vamos aplicar o procedimento de ortogonalização em um modelo de machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "\n",
    "# Criar a variável de resultado\n",
    "df['Y'] = df['bweight']\n",
    "\n",
    "# Criar a variável 'Treated' com valor 1 se 'mbsmoke' for 'smoker', caso contrário 0\n",
    "df['D'] = np.where(df['mbsmoke'] == 'smoker', 1, 0)\n",
    "\n",
    "# Criar a variável 'casada' com valor 1 se 'mmarried' for 'married', caso contrário 0\n",
    "df['casada'] = np.where(df['mmarried'] == 'married', 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desviar este conjunto de dados, precisaremos de dois modelos. O primeiro modelo, vamos chamá-lo $M_{D}(X)$, prevê o tratamento (Se a gestante é fumante, no nosso caso) utilizando os confundidores. É um dos estágios que vimos acima, no teorema de Frisch–Waugh–Lovell.\n",
    "\n",
    "Assim que tivermos este modelo, construiremos os resíduos\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - M_{D}(X_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_D = smf.ols(\"D ~ 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['D_star'] = df['D'] - m_D.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode pensar neste resíduo como uma versão do tratamento que é imparcial ou, melhor ainda, que é impossível de prever a partir dos fatores de confusão $X$. Como os fatores de confusão já eram usados ​​para prever $D$, o resíduo é, por definição, imprevisível com com $X$. Outra maneira de dizer isso é que o viés foi explicado pelo modelo $M_{D}(X)$, produzindo $D_{i}^{*}$ que é tão bom quanto atribuído aleatoriamente. É claro que isso só funciona se tivermos em $X$ todos os fatores de confusão que causam ambos $D$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos construir resíduos para o resultado.\n",
    "\n",
    "$$ y_{i}^{*} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "\n",
    "Este é outro estágio do teorema de Frisch – Waugh – Lovell. Isso não torna o conjunto menos tendencioso, mas facilita a estimativa do efeito, reduzindo a variância em $y$. Mais uma vez você pode pensar $y_{i}^{*}$ como uma versão de $y_{i}$ imprevisível de $X$ ou que teve todas as suas variações devido a $X$ explicadas. Pense nisso. Nós já usamos $X$ para prever $y$ com $M_{y}(X_{i})$. E $y_{i}^{*}$ é o erro dessa previsão. Então, por definição, não é possível prever isso a partir de $X$. Todas as informações em $X$ para prever $y$ já foram usadas. Se for esse o caso, a única coisa que resta para explicar $y_{i}^{*}$ é algo que não usamos usamos para construí-lo (não incluído em $X$), que é apenas o tratamento (novamente, assumindo que não há fatores de confusão não medidos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_y = smf.ols(\"Y ~  1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['y_star'] = df['Y'] - m_y.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, aplicando o teorema para o segundo estágio,\n",
    "\n",
    "$$ y_{i}^{*} = \\alpha_{0} + \\beta_{2} D_{i}^{*} + e_{i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_star   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                  0.021\n",
      "Method:                 Least Squares   F-statistic:                     100.6\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           1.91e-23\n",
      "Time:                        18:44:43   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.172e+04\n",
      "Df Residuals:                    4640   BIC:                         7.173e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.586e-11      8.041   1.97e-12      1.000     -15.764      15.764\n",
      "D_star      -220.0220     21.932    -10.032      0.000    -263.020    -177.024\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         2.73\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL1 = smf.ols(\"y_star ~ D_star\", data=df).fit()\n",
    "print(FWL1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar o resultado com o OLS tradicional que considera diretamente todas as covariáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.104\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     38.49\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):          5.80e-100\n",
      "Time:                        18:44:49   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.175e+04\n",
      "Df Residuals:                    4627   BIC:                         7.184e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2851.5203     54.983     51.861      0.000    2743.727    2959.314\n",
      "D           -220.0220     21.963    -10.018      0.000    -263.080    -176.964\n",
      "casada        42.7633     23.589      1.813      0.070      -3.482      89.009\n",
      "mage           3.2878      1.935      1.699      0.089      -0.506       7.082\n",
      "medu          -1.5659      4.130     -0.379      0.705      -9.664       6.532\n",
      "fhisp        -22.9258     63.841     -0.359      0.720    -148.084     102.232\n",
      "mhisp         15.0538     68.152      0.221      0.825    -118.557     148.664\n",
      "foreign       -6.8607     39.628     -0.173      0.863     -84.550      70.828\n",
      "alcohol       -9.5560     46.486     -0.206      0.837    -100.690      81.578\n",
      "deadkids     -13.8988     18.872     -0.736      0.461     -50.897      23.099\n",
      "nprenatal     26.1331      2.328     11.225      0.000      21.569      30.697\n",
      "mrace        176.3951     44.950      3.924      0.000      88.272     264.518\n",
      "frace         50.9355     44.172      1.153      0.249     -35.663     137.534\n",
      "fage          -0.9152      1.194     -0.767      0.443      -3.256       1.425\n",
      "fedu           1.0262      3.139      0.327      0.744      -5.128       7.180\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         485.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols = smf.ols(\"Y ~ D + 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente o teorema de Frisch-Waugh-Lovell funciona mesmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que depois de fazermos as duas transformações, a única coisa que resta para prever esses resíduos é o tratamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resumir, ao prever o tratamento, construímos $D^{*}$ que funciona como uma versão imparcial do tratamento; ao prever o resultado, construímos $y^{*}$ que é uma versão do resultado que só pode ser explicada se usarmos o tratamento. Esses dados, onde substituímos por $y$ por $y^{*}$ e $D$ por $D^{*}$, são os dados desviados que queríamos. Podemos usá-lo para avaliar nosso modelo causal da mesma forma que fizemos anteriormente, usando dados aleatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DML - Orthogonal/Double Machine Learning\n",
    "\n",
    "* Double Machine Learning - DML\n",
    "* Double/Debiased Machine Learning - DDML \n",
    "\n",
    "No exemplo anterior vimos que o teorema de Frisch-Waugh-Lovell nos permite estimar o efeito causal de um tratamento $D$ sobre um resultado $Y$ enquanto controlamos por um conjunto de covariáveis $X$ para uma abordagem linear nos dois estágios.\n",
    "\n",
    "No entanto, a abordagem linear pode ser muito restritiva. Em muitos casos, as relações entre as variáveis podem ser não lineares ou de alta dimensão. Nesses casos, podemos usar **algoritmos de aprendizado de máquina** para **descobrir essas relações não lineares de alta dimensão**. Nesse sentido, o artigo de **Chernozhukov et al (2018)**  mostra que também é possível fazer ortogonalização com modelos de aprendizado de máquina. Vejamos a ideia básica - Chamado de modelo \"Partially Linear Regression Models (PLR)\".\n",
    "\n",
    "### Partially Linear Regression Models (PLR)\n",
    "\n",
    "O modelo PLR é usado para problemas de inferência causal com um tratamento $D$ e um resultado $Y$, assumindo que a relação entre $D$ e $Y$ é linear, enquanto outras covariáveis $X$ podem ter relações não lineares com $Y$ ou $D$.\n",
    "\n",
    "$$ Y_{i} = \\theta D_{i} + M(X_{i}) + \\epsilon_{i}$$\n",
    "\n",
    "Então, o modelo PLR é uma extensão do modelo de regressão linear tradicional, onde a relação entre $D$ e $Y$ é linear, mas a relação entre $X$ e $Y$ ou $D$ pode ser não linear. Seguindo o procedimento do FWL, obtemos os resíduos:\n",
    "\n",
    "$$ y_{i}^{*} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - M_{D}(X_{i})$$\n",
    "\n",
    "onde $M_{y}$ e $M_{D}$ são modelos não lineares que podem ser obtidos por algorítmos de aprendizado de máquina. A ideia é que o conjunto de recursos ($X$) prevê o resultado ($Y$) e o tratamento ($D$) e, por alguma função não linear.\n",
    "\n",
    "* **Conceitos importantes relacionados**\n",
    "  * ***Machine Learning (ML)* e *Overfitting*** : Os modelos de aprendizado de máquina (ML) podem ajustar-se perfeitamente aos dados, ou melhor, superajustá-los (*Overfitting* / \"sobreajuste\"). \n",
    "\n",
    "Apenas olhando para as equações anteriores, podemos saber o que acontecerá nesse caso.\n",
    "  * Se $M_{y}$ de alguma forma superajusta, os resíduos serão todos muito próximos de zero. Se isso acontecer, será difícil descobrir como $D$ afeta isso. \n",
    "  * Se $M_{D}$ de alguma forma superajusta, seus resíduos também serão próximos de zero. \n",
    "  * Conseqüentemente, não haverá variação no resíduo do tratamento ($D$) para ver como isso pode impactar o resultado ($Y$).\n",
    "\n",
    "O que fazer em caso de *Overfitting*?\n",
    "\n",
    "* **Regularização**: técnica que adiciona um termo à função de perda que penaliza os coeficientes do modelo. Isso faz com que o modelo seja menos sensível aos dados de treinamento, evitando o superajuste.\n",
    "\n",
    "Quais os modelos de ML mais comuns para previsão?\n",
    "\n",
    "* Random Forest: é um modelo de aprendizado de máquina que pode ser usado tanto para classificação quanto para regressão. Ele é um modelo de conjunto que treina várias árvores de decisão em subconjuntos aleatórios dos dados e faz a média de suas previsões.\n",
    "* Gradient Boosting: é um modelo de aprendizado de máquina que constrói um modelo aditivo de forma progressiva. Ele permite a otimização de funções de perda diferenciáveis arbitrárias.\n",
    "* Redes Neurais: são modelos de aprendizado de máquina que são inspirados na forma como o cérebro humano funciona. Eles são compostos por camadas de neurônios que processam e transmitem informações.\n",
    "* Support Vector Machines: são modelos de aprendizado de máquina que são usados para classificação e regressão. Eles são eficazes em espaços de alta dimensão e são capazes de lidar com dados não lineares.\n",
    "* Outros modelos de ML mais usados em economia são: LASSO, Ridge, Elastic Net, etc. LASSO é um método de regressão que adiciona uma penalidade L1 à função de perda. Isso faz com que alguns coeficientes sejam exatamente zero, o que é útil para seleção de recursos. Ridge é um método de regressão que adiciona uma penalidade L2 à função de perda. Isso faz com que os coeficientes sejam menores, o que é útil para reduzir a variância. Elastic Net é um método de regressão que combina as penalidades L1 e L2. Isso permite que você selecione recursos e reduza a variância ao mesmo tempo.\n",
    "\n",
    "**Validação Cruzada (Cross-Validation)**:  é uma técnica estatística usada para avaliar a performance de um modelo preditivo e sua capacidade de generalização para novos dados. Em essência, é um método de dividir os dados disponíveis em partes para treinar e testar o modelo de maneira eficiente e confiável.\n",
    "\n",
    "* **Validação Cruzada K-fold**: é uma técnica de validação cruzada que divide os dados em k partes. O modelo é treinado em k-1 partes e testado na parte restante. Isso é feito k vezes, de modo que cada parte seja usada como conjunto de teste exatamente uma vez.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\kfold-cv.png\"  alt=\"Imagem\" style=\"width: 450px;\"/>\n",
    "</div>\n",
    "\n",
    "Felizmente, esse tipo de validação cruzada é muito fácil de implementar usando `cross_val_predict` função do Sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicações do DML-Manual para identificar o ATE\n",
    "\n",
    "Visto esses conceitos básicos, vamos voltar ao DML. Especialmente, vamos aplicar o DML considerando dois modelos de machine learning. Primeiramente consideramos o Random Forest e posteriormente o Gradient Boosting.\n",
    "\n",
    "1. Aplicação do DML com o Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "\n",
    "folds = 10\n",
    "np.random.seed(123)\n",
    "\n",
    "m_D1 = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "D_res1 = df[D] - cross_val_predict(m_D1, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y1 = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "y_res1 = df[y] - cross_val_predict(m_y1, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res1   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.014\n",
      "Method:                 Least Squares   F-statistic:                     67.90\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           2.22e-16\n",
      "Time:                        18:47:35   Log-Likelihood:                -36188.\n",
      "No. Observations:                4642   AIC:                         7.238e+04\n",
      "Df Residuals:                    4640   BIC:                         7.239e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     22.8144      8.862      2.574      0.010       5.441      40.188\n",
      "D_res1      -163.7569     19.873     -8.240      0.000    -202.718    -124.796\n",
      "==============================================================================\n",
      "Omnibus:                      327.610   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              742.618\n",
      "Skew:                          -0.446   Prob(JB):                    5.53e-162\n",
      "Kurtosis:                       4.744   Cond. No.                         2.33\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML1 = smf.ols(\"y_res1 ~ D_res1\", data=df).fit()\n",
    "print(FWL_DML1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Aplicação do DML com o Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Definir as variáveis X, D, y\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "\n",
    "# Definir o número de folds para a validação cruzada\n",
    "folds = 10\n",
    "np.random.seed(123)\n",
    "\n",
    "# Garantir reprodutibilidade nos modelos de Gradient Boosting\n",
    "m_D2 = GradientBoostingClassifier(n_estimators=100, random_state=123)\n",
    "D_res2 = df[D] - cross_val_predict(m_D2, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y2 = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "y_res2 = df[y] - cross_val_predict(m_y2, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O segundo estágio do DML é estimar o efeito do tratamento no resultado. Para isso, usamos um modelo de regressão linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res2   R-squared:                       0.016\n",
      "Model:                            OLS   Adj. R-squared:                  0.016\n",
      "Method:                 Least Squares   F-statistic:                     77.25\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           2.08e-18\n",
      "Time:                        18:47:48   Log-Likelihood:                -35887.\n",
      "No. Observations:                4642   AIC:                         7.178e+04\n",
      "Df Residuals:                    4640   BIC:                         7.179e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     24.4239      8.533      2.862      0.004       7.695      41.153\n",
      "D_res2      -175.3738     19.953     -8.789      0.000    -214.491    -136.257\n",
      "==============================================================================\n",
      "Omnibus:                      473.532   Durbin-Watson:                   1.987\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1208.288\n",
      "Skew:                          -0.586   Prob(JB):                    4.20e-263\n",
      "Kurtosis:                       5.207   Cond. No.                         2.52\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML2 = smf.ols(\"y_res2 ~ D_res2\", data=df).fit()\n",
    "print(FWL_DML2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos a mecânica do DML. Com o uso de modelos de ML não lineares.\n",
    "\n",
    "Na aula extra (13-xtr_ML_overf_CV.ipynb), você pode verificar como escolher o melhor modelo de ML (procedimentos de validação cruzada, e critérios de seleção de modelos).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicações do DML através de pacotes para identificar o ATE\n",
    "\n",
    "**EconML**\n",
    "\n",
    "Vamos utilizar o pacote EconML para aplicar o DML de forma mais eficiente. Esse pacote aplica Validação Cruzada Adaptativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -210.026      26.169 -8.026    0.0      -261.315      -158.736\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "  120.872        -397.661         131.258\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     123.672       -450.408        161.909\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# Definir as variáveis\n",
    "X = df[['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']]\n",
    "D = df['D']\n",
    "y = df['Y']\n",
    "\n",
    "# Converter variáveis categóricas em dummies (se necessário)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Definir os modelos de Machine Learning para (i) X em Y, e (ii) X em D\n",
    "model_y = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "model_d = GradientBoostingClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "# Criar o estimador LinearDML\n",
    "estimator = LinearDML(model_y=model_y,\n",
    "                      model_t=model_d,\n",
    "                      discrete_treatment=True,\n",
    "                      cv=10,\n",
    "                      random_state=123)\n",
    "\n",
    "# Ajustar o modelo\n",
    "estimator.fit(y, D, X=X)\n",
    "\n",
    "ate_inf = estimator.ate_inference(X=X)\n",
    "print(ate_inf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os resultados não são identicos porque No método manual com cross_val_predict, estamos usando validação cruzada *k-fold* para calcular os resíduos. O cross_val_predict faz predições out-of-sample para cada observação, mas: \n",
    "* Ele não ajusta automaticamente os hiperparâmetros dos modelos.\n",
    "* Ele aplica a validação cruzada de forma isolada para Y e D, sem sincronizar as divisões.\n",
    "\n",
    "O LinearML, por outro lado:\n",
    "* Usa validação cruzada para ambos os modelos $M_{y}$ e $M_{D}$, mas com divisões sincronizadas, garantindo que os conjuntos de treino e teste sejam consistentes entre Y e D.\n",
    "* Possui ajustes internos que minimizam o viés e a variância nos resíduos.\n",
    "\n",
    "Essas diferenças podem levar a discrepâncias nos resíduos $y^{*}$ e $D^{*}$, e, consequentemente, nos coeficientes estimados.\n",
    "\n",
    "Quanto aos erros padrão do segundo estágio, \n",
    "* No método manual, o segundo estágio (regressão linear com smf.ols) assume que os resíduos $Y^{*}$ e $D^{*}$ são independentes e identicamente distribuídos (i.i.d.). Isso pode subestimar ou superestimar a variância de $\\theta$.\n",
    "* O LinearDML ajusta os erros padrão para levar em conta:\n",
    "  * A incerteza nos modelos do primeiro estágio ($M_{y}$ e $M_{D}$).\n",
    "  * A possível correlação entre $Y^{*}$ e $D^{*}$ devido a predições do primeiro estágio.\n",
    "\n",
    "Esse ajuste no LinearDML pode resultar em coeficientes ligeiramente diferentes e intervalos de confiança mais amplos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DoubleML**\n",
    "\n",
    "Aplicando a biblioteca DoubleML, que é uma biblioteca Python para Double Machine Learning. O DoubleML é uma implementação eficiente e flexível do Double/Debiased Machine Learning (DML) para estimar efeitos de tratamento heterogêneos em Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: [-214.39535587]\n",
      "Standard Error: [22.72757171]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Definir as variáveis\n",
    "X = df[['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']]\n",
    "D = df['D']\n",
    "y = df['Y']\n",
    "\n",
    "# Converter variáveis categóricas em dummies (se necessário)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Criar o objeto DoubleMLData\n",
    "data = DoubleMLData(pd.concat([X, D, y], axis=1), y_col='Y', d_cols='D', x_cols=list(X.columns))\n",
    "\n",
    "# Modelos de Machine Learning para (i) Y | X e (ii) D | X\n",
    "learner_y = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "learner_d = GradientBoostingClassifier(n_estimators=100, random_state=123)\n",
    "learner_l = LinearRegression()  # Modelo linear para o segundo estágio\n",
    "\n",
    "# Configurar validação cruzada (10-fold)\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=123)\n",
    "\n",
    "# Criar o modelo DoubleMLPLR (Partial Linear Regression equivalente ao LinearDML)\n",
    "dml_plr = DoubleMLPLR(data, ml_m=learner_d, ml_l=learner_l, n_folds=10)\n",
    "\n",
    "# Ajustar o modelo\n",
    "dml_plr.fit()\n",
    "\n",
    "# Obter os resultados do ATE\n",
    "ate = dml_plr.coef\n",
    "stderr = dml_plr.se\n",
    "\n",
    "print(f\"ATE: {ate}\")\n",
    "print(f\"Standard Error: {stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O DoubleML e o EconML são pacotes que implementam o DML. Os resultados são praticamente os mesmos. \n",
    "\n",
    "O DoubleML é mais flexível e permite a implementação de modelos de primeiro e segundo estágio personalizados. O EconML é mais fácil de usar e possui mais funcionalidades prontas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outros Modelos de DML para identificar o ATE\n",
    "\n",
    "Entretanto, existem possíveis desdobramentos do modelo visto anteriormente. Por exemplo, outro modelo possível seria o modelo \" Partially Linear IV Regression Models\" (PLIV).\n",
    "\n",
    "**Partially Linear IV Regression Models (PLIV)**\n",
    "\n",
    "Ele é usado quando $D$ é endógeno, ou seja, correlacionado com o erro $\\epsilon$.\n",
    "\n",
    "$$ Y = \\theta D + g(X) + \\epsilon$$\n",
    "\n",
    "$$ D = m(X,Z) + \\eta$$\n",
    "\n",
    "Com $Z$ sendo as variáveis instrumentais. Repare que os instrumentos entram na primeira etapa, para desviar o tratamento $D$.\n",
    "\n",
    "\n",
    "\n",
    "**Interactive regression models (IRM)**\n",
    "\n",
    "O modelo IRM **relaxa a suposição de linearidade** entre $D$ e $Y$. Ele permite que o efeito do tratamento $D$ seja não linear e interativo com $X$:\n",
    "\n",
    "$$ Y = g(D,X) + U $$\n",
    "\n",
    "$$ D = m(X) + V $$\n",
    "\n",
    "Tem como objetivo relaxar a suposição de separabilidade entre $X$ e $D$, permitindo interações entre as covariáveis $X$ e o tratamento $D$.\n",
    "\n",
    "Procedimento:\n",
    "* Estima-se $E[D|X]$\n",
    "* Calcula-se $E[Y|X,D=0]$ e $E[Y|X,D=1]$, permitindo modelar como $Y$ depende de $D$ e $X$ conjuntamente.\n",
    "* Suposição de D binário, e relações não lineares e interativas podem existir entre $X$, $D$ e $Y$.\n",
    "\n",
    "**Interactive IV regression models (IIVM)**\n",
    "\n",
    "* Modelo\n",
    "\n",
    "$$ Y = g(D,X) + U $$\n",
    "$$ D = m(X,Z) + V $$\n",
    "$$ Z = h(X) + E $$\n",
    "\n",
    "objetivo é Estimar o efeito médio local do tratamento (LATE) em contextos com endogeneidade e relações interativas entre $Z$, $X$, $D$ e $Y$.\n",
    "\n",
    "Procedimento, Estima-se:\n",
    "* $E[Y|X,Z=0]$, $E[Y|X,Z=1]$: Relação entre $Y$, $X$ e $Z$\n",
    "* $E[D|X,Z=0]$, $E[D|X,Z=1]$: Relação entre $D$, $X$ e $Z$\n",
    "* $E[Z|X]$: Relação entre $Z$ e $X$\n",
    "\n",
    "Usa-se $Z$ para capturar interações entre as variáveis. Supõe-se relação não linear e iterativa entre $D$, $Z$, $X$ e $Y$. E $Z$ é um instrumento válido. Aplicação do LATE em situações de endogeneidade com heterogeneidade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arcabouço de Resultados Potenciais, Individual Treatment Effects (ITE) e Conditional Average Treatment Effects (CATE)\n",
    "\n",
    "\n",
    "**Individual Treatment Effect - ITE - $\\beta_{i}$**\n",
    "\n",
    "Podemos definir o efeito do tratamento individual (ITE) como a diferença entre os resultados potenciais. \n",
    "\n",
    "$$ \n",
    "\\beta_{i}^{ITE} = Y_{i}(1) - Y_{i}(0) \n",
    "$$\n",
    "\n",
    "Segundo o problema fundamental da inferência causal, nunca podemos observar o mesmo indivíduo sob diferentes condições de tratamento. \n",
    "\n",
    "$$\n",
    "Y^{obs}_i(D)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } D=1 \\\\\n",
    "Y_i(0), & \\text{se } D=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "**Average Treatment Effect - ATE**\n",
    "\n",
    "Como estávamos acostumados, pode-se definir o efeito médio do tratamento como:\n",
    "\n",
    "$$\n",
    "\\beta^{ATE}= E[Y_i(1) − Y_i(0)] = E [ \\beta_{i}^{ITE} ] = E[\\beta_i]\n",
    "$$\n",
    "\n",
    "**Conditional Average Treatment Effect - CATE**\n",
    "\n",
    "\n",
    "\n",
    "$$ \\beta^{CATE}(x) = E[Y_i(1) − Y_i(0)|X] = E[\\beta_i|X_{i}=x] $$\n",
    "\n",
    "O CATE é a média do efeito do tratamento para indivíduos com características específicas representadas pelas covariáveis $X$.\n",
    "\n",
    "Repare que **os ITE são inerentemente não observáveis.** Entretanto, o que pode ser estimado, em vez disso, é o **Conditional Average Treatment Effect (CATE)**. Ou seja, o efeito esperado do tratamento individual, condicional em covariáveis $​​X$. \n",
    "\n",
    "Vejamos os pressupostos necessários para identificação do efeito causal.\n",
    "\n",
    "\n",
    "**Hipóteses de Identificação do CATE**\n",
    "\n",
    "São as condições necessárias para garantir que possamos identificar e estimar o CATE usando dados observacionais. Essas hipóteses estabelecem como o efeito causal pode ser extraído, evitando viés e problemas de endogeneidade. \n",
    "\n",
    "* **Não Confundimento** / Inconfundibilidade (Unconfoundedness): Justifica que todas as diferenças entre tratados e não tratados são capturadas pelas covariáveis\n",
    "\n",
    "$$ Y_{i}(0), Y_{i}(1) \\perp T|X $$\n",
    "\n",
    "* **Sobreposição** (Overlap): Garante que temos dados para comparar indivíduos semelhantes em ambos os grupos.\n",
    "  * Para cada valor das covariáveis $X$, deve haver unidades suficientes tanto no grupo tratado quanto no grupo controle. Sem essa suposição, não seria possível comparar grupos semelhantes sob diferentes níveis de tratamento.\n",
    "  \n",
    "$$ 0 < P(D_{i}=1| X_{i}=x) < 1 $$\n",
    "\n",
    "\n",
    "* **Consistência** (Consistency): A suposição de consistência estabelece que o resultado observado ($Y_{i}^{obs}$) para uma unidade corresponde ao resultado potencial associado ao tratamento efetivamente recebido:\n",
    "\n",
    "$$\n",
    "Y^{obs}_i(D)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } D=1 \\\\\n",
    "Y_i(0), & \\text{se } D=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Essa hipótese conecta os resultados observados aos contrafactuais de forma coerente e supõe que:\n",
    "* Não há múltiplas versões do tratamento ($D$) que possam gerar diferentes resultados potenciais.\n",
    "* O tratamento recebido por uma unidade não afeta os resultados de outras unidades (parte do SUTVA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimação do CATE com DML\n",
    "\n",
    "Até agora, vimos como o Double/Debiased ML (Double Machine Learning - DML) nos permite focar na estimativa do Efeito Médio do Tratamento (ATE). \n",
    "\n",
    "No entanto, ele também pode ser usado para estimar a **heterogeneidade dos efeitos do tratamento** ou o **Efeito Médio Condicional do Tratamento (CATE)**. Ou seja, em vez de considerar um efeito de tratamento constante para todas as observações, o modelo ajustado permite um efeito diferente com base em suas características específicas. Isto é, enquanto o ATE assume que o tratamento tem o mesmo efeito em todos os indivíduos, o CATE permite que o efeito varie de acordo com as características individuais, capturadas por $X$.\n",
    "\n",
    "Essa ideia é trabalhada no artigo de Chernozhukov et al (2018), que propõe uma abordagem para estimar o CATE usando DML.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\Chernozhucov.png\"  alt=\"Imagem\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Partindo do Partially Linear Regression Models (PLR), Chernozhukov et al (2018) propõem uma abordagem para estimar o CATE.\n",
    "\n",
    "$$ Y = \\theta(X)D + g_{y}(X) + \\epsilon$$\n",
    "\n",
    "* $\\theta(X)$ é o efeito heterogêneo do tratamento (CATE).\n",
    "* $g_{y}(X)$ é uma função de $X$ que captura a relação entre $Y$ e $X$ (ou seja, os fatores não relacionados ao tratamento).\n",
    "* $E(\\epsilon|D,X)=0$, e\n",
    "\n",
    "$$ D = m_{D}(X) + \\eta$$\n",
    "\n",
    "* $m_{D}(X)$ é uma função de $X$ que captura a relação entre $D$ e $X$\n",
    "* $E(\\eta|X)=0$.\n",
    "\n",
    "Isso significa que o efeito do tratamento ($\\theta(X)$) é uma função de $X$, e não um valor constante. Além disso,  tanto o tratamento ($D$) quanto o resultado ($Y$) são funções de $X$.\n",
    "\n",
    "\n",
    "Utilizando o DML, podemos estimar o CATE seguindo os seguintes passos:\n",
    "\n",
    "Com base no procedimento de ortogonalização, os primeiros estágios do DML são:\n",
    "\n",
    "$$ D^{*} = D - m_{0}(X)$$\n",
    "\n",
    "e,\n",
    "\n",
    "$$ Y^{*} = Y - g_{y}(X)$$\n",
    "\n",
    "Posteriormente, teremos o segundo estágio do DML:\n",
    "\n",
    "$$ Y^{*} = \\alpha + \\theta(X)D^{*} + \\epsilon$$\n",
    "\n",
    "\n",
    "No primeiro estágio do DML, ajustamos modelos para $D$ e $Y$ em função de $X$, gerando os resíduos $D^{∗}$ e $Y^{∗}$. Esses resíduos removem a dependência de $X$, permitindo que a segunda etapa estime o efeito causal sem viés. Então, regredindo $Y^{*}$ em $D^{*}$, obtemos o CATE.\n",
    "\n",
    "$$ \\theta = argmin_{\\theta} E[(Y^{*} - \\theta (X) D^{*})^{2}] + \\lambda R(\\theta)$$\n",
    "\n",
    "para algum termo de regularização fortemente convexo $R$, e $\\lambda > 0$ é um parâmetro de regularização. O termo de regularização ajuda a evitar overfitting e é especialmente útil em modelos de alta dimensão, onde $\\theta$ pode ser esparso (muitos coeficientes iguais a zero, em outras palavras, apenas algumas variáveis ou características têm influência significativa, enquanto o restante pode ser ignorado sem perda relevante de informação).\n",
    "\n",
    "* Chernozhukov et al (2016) considera o caso em que $\\theta(X)$ é uma constante (ATE), ou uma função linear de $X$ (CATE) de baixa dimensão.\n",
    "* Nie (2017) cai em um Espaço de Hilbert do Kernel Reprodutor (RKHS).\n",
    "* Chernozhukov et al (2018) consideram o caso de um espaço linear esparso de alta dimensão, onde $\\theta(X) = <\\theta, \\phi(X)>$ para algum mapeamento de características de alta dimensão conhecido e onde $\\theta$ tem muito poucas entradas diferentes de zero (esparsas)\n",
    "* Athey (2019), entre outros resultados, considera o caso em que $\\theta(X)$ é uma função lipschitz não paramétrica e usa modelos de floresta aleatória para ajustar a função. Esse métodos permite maior flexibilidade para modelar $\\theta(X)$ sem assumir linearidade.\n",
    "* Foster (2019) permite modelos arbitrários $\\theta(X)$ e fornecer resultados com base em medidas de complexidade de amostra do espaço do modelo (por exemplo, complexidade de Rademacher, entropia métrica). Esse métodos permite maior flexibilidade para modelar $\\theta(X)$ sem assumir linearidade.\n",
    "\n",
    "A principal vantagem do DML é que se fizermos suposições paramétricas sobre $\\theta(X)$, então se obtém taxas de estimativa rápidas e, para muitos casos de estimadores de estágio final, também normalidade assintótica na estimativa do segundo estágio $\\hat{\\theta}$.\n",
    "\n",
    "**Estimativas do CATE**\n",
    "\n",
    "O ponto novo na nossa abordagem é que, agora, o $\\theta(X)$ é o efeito heterogêneo do tratamento (CATE), e assumimos que ele segue uma relação linear das covariáveis/recursos (a linearidade assumida para $\\theta(X)$ é uma simplificação útil para interpretação e análise inicial), da seguinte forma:\n",
    "\n",
    "$$ \\theta(X) = X'\\beta + \\theta_{\\text{intercept}}$$\n",
    "\n",
    "* $X'$ é o vetor transposto das covariáveis.\n",
    "* $\\beta$ são os coeficientes que medem o efeito de cada covariável em $\\theta(X)$.\n",
    "* $\\theta_{\\text{intercept}}$ é o intercepto do modelo (cate_intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo anterior eu havia calculado o efeito médio. Agora, vamos calcular o efeito heterogêneo do tratamento (CATE) para cada característica. Os resultados já estão disponíveis no objeto \"estimator\" realizado pelo pacote EconML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Coefficient Results                       \n",
      "================================================================\n",
      "          point_estimate  stderr zstat  pvalue ci_lower ci_upper\n",
      "----------------------------------------------------------------\n",
      "casada            28.828  55.792  0.517  0.605  -80.522  138.178\n",
      "mage             -11.333   5.261 -2.154  0.031  -21.644   -1.022\n",
      "medu              17.987  11.957  1.504  0.133   -5.449   41.422\n",
      "fhisp             66.089 130.914  0.505  0.614 -190.497  322.676\n",
      "mhisp            -85.452 161.644 -0.529  0.597 -402.269  231.364\n",
      "foreign           348.68 153.471  2.272  0.023   47.882  649.478\n",
      "alcohol          -30.274  91.655  -0.33  0.741 -209.915  149.368\n",
      "deadkids          37.647  51.511  0.731  0.465  -63.312  138.606\n",
      "nprenatal        -14.287   6.922 -2.064  0.039  -27.853   -0.721\n",
      "mrace           -144.841 102.649 -1.411  0.158 -346.029   56.347\n",
      "frace             48.413  99.684  0.486  0.627 -146.964  243.791\n",
      "fage              -0.022   2.781 -0.008  0.994   -5.472    5.427\n",
      "fedu              -0.491   8.399 -0.059  0.953  -16.954   15.971\n",
      "                       CATE Intercept Results                       \n",
      "====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower ci_upper\n",
      "--------------------------------------------------------------------\n",
      "cate_intercept         57.711 166.066 0.348  0.728 -267.772  383.194\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n"
     ]
    }
   ],
   "source": [
    "print(estimator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: O DoubleML não fornece diretamente um método para estimar o CATE de forma paramétrica como no LinearDML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretação:**\n",
    "\n",
    "* mage (idade da mãe):\n",
    "  * point_estimate: -11.333 (pvalue: 0.031)\n",
    "  * Isso sugere que, para cada aumento de um ano na idade da mãe, o efeito negativo de fumar durante a gravidez no peso ao nascer diminui em 11.047 gramas (ou seja, o efeito se torna mais negativo).\n",
    "  * Há evidência estatística de que a idade da mãe influencia o efeito de fumar durante a gravidez sobre o peso ao nascer.\n",
    "\n",
    "* medu (educação materna):\n",
    "  * point_estimate: 17.987 (pvalue: 0.015)\n",
    "  * Para cada ano adicional de educação da mãe, o efeito negativo de fumar durante a gravidez no peso ao nascer é reduzido em 26.091 gramas (o efeito negativo é mitigado).\n",
    "  * A educação materna parece reduzir o impacto negativo de fumar durante a gravidez.\n",
    "\n",
    "* foreign (se a mãe é estrangeira):\n",
    "  * point_estimate: 435.58 (pvalue: 0.001 (altamente significativo))\n",
    "  * Interpretation: Mães estrangeiras têm um efeito tratamento condicional que é 435.58 gramas maior do que o de mães não estrangeiras.\n",
    "  * A origem estrangeira da mãe está associada a uma redução significativa do efeito negativo de fumar durante a gravidez.\n",
    "\n",
    "* Outras variáveis: Algumas covariáveis não são estatisticamente significativas (pvalue > 0.05), indicando que não há evidência suficiente para afirmar que essas covariáveis influenciam o efeito do tratamento.\n",
    "\n",
    "* CATE Intercept Results (Resultados do Intercepto do CATE):\n",
    "  * cate_intercept: 7.624\n",
    "  * Este é o valor base do efeito tratamento condicional quando todas as covariáveis estão em zero. Como zero pode não ser um valor interpretável para algumas covariáveis (por exemplo, idade da mãe), o intercepto isoladamente pode não ter uma interpretação prática direta.\n",
    "\n",
    "* OBS: Covariáveis contínuas vs. categóricas: Para variáveis contínuas (como mage), o coeficiente representa a variação no efeito do tratamento por unidade de aumento na covariável. Para variáveis binárias (como foreign), o coeficiente representa a diferença no efeito do tratamento entre os grupos (por exemplo, estrangeiras vs. não estrangeiras).\n",
    "\n",
    "\n",
    "**Resumo:**\n",
    "\n",
    "* O modelo estima que o efeito do tratamento (fumar durante a gravidez) sobre o peso ao nascer não é constante, mas varia linearmente com as covariáveis $X$. O sinal e magnitude dos coeficientes indicam a direção e a intensidade com que cada covariável afeta o efeito do tratamento.\n",
    "* O impacto de fumar durante a gravidez no peso ao nascer não é o mesmo para todas as mães; varia de acordo com características como idade, educação e nacionalidade.\n",
    "* Os resultados sugerem que políticas públicas visando reduzir o tabagismo durante a gravidez podem ser mais eficazes se levarem em consideração essas características. Por exemplo, focar em mães mais jovens ou com menor nível educacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como estimar o efeito do tratamento para um indivíduo com uma característica específica?**\n",
    "\n",
    "Para calcular o efeito tratamento condicional ($\\theta(X)$) para um conjunto específico de covariáveis, você pode usar a fórmula:\n",
    "\n",
    "$$ \\hat{\\theta}(X) = X'\\hat{\\beta} + \\hat{\\theta}_{\\text{intercept}}$$\n",
    "\n",
    "Por exemplo, suponha que você queira calcular o efeito para uma mãe com as seguintes características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate   stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                            \n",
      "0         -253.02  101.255 -2.499   0.012  -451.476   -54.564\n"
     ]
    }
   ],
   "source": [
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Certificar-se de que as colunas correspondem às usadas no modelo\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular o efeito e obter a inferência\n",
    "effect_inf = estimator.effect_inference(X_new)\n",
    "print(effect_inf.summary_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o perfil de mãe especificado, fumar durante a gravidez está associado a uma redução média de aproximadamente 253.02 gramas no peso ao nascer do bebê. Logo, há evidência estatística significativa de que fumar durante a gravidez está associado a uma redução substancial no peso ao nascer do bebê."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes do DML**\n",
    "\n",
    "**Modelos Lineares**\n",
    "\n",
    "* LinearDML: usa um **modelo linear** final **não regularizado** e funciona essencialmente apenas quando o vetor de características $X$ é de baixa dimensão. Oferece intervalos de confiança por meio de argumentos de normalidade assintótica. Também é possível construir intervalos de confiança baseados em bootstrap definindo inference='bootstrap' (Chernozhukov, 2016).\n",
    "* SparseLinearDML: é uma extensão do LinearDML que usa **regularização L1** para lidar com a alta dimensionalidade de $X$. Usa uma implementação do algoritmo DebiasedLasso, propriedades de normalidade assintótica do DebiasedLasso, esta classe também oferece intervalos de confiança assintoticamente normais (Chernozhukov, 2017; Chernozhukov, 2018). \n",
    "* KernelDML: é uma classe que usa **kernel ridge regression** para estimar o efeito do tratamento (RKHS - Nie, 2017). Ela aproxima qualquer função no RKHS criando recursos aleatórios de Fourier. Em seguida, executa um modelo final regularizado ElasticNet. Além disso, dado que usamos Recursos aleatórios de Fourier, esta classe assume um kernel RBF.\n",
    "\n",
    "\n",
    "**Modelos Não-Lineares**\n",
    "\n",
    "* NonParamDML: não faz nenhuma suposição sobre o modelo de efeito para cada resultado $i$. No entanto, ele se aplica somente quando o tratamento é binário ou unidimensional contínuo.\n",
    "* CausalForestDML: usa uma Floresta Causal como um modelo final (Wager, 2018; Athey, 2019). Este estimador oferece intervalos de confiança via *Bootstrap-of-Little-Bags* conforme descrito em (Athey, 2019) . Usando esta funcionalidade, também podemos construir intervalos de confiança para o CATE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos mais flexiveis (Não-Lineares)**\n",
    "\n",
    "Se a heterogeneidade do efeito não tiver uma forma linear, então essa abordagem anterior não é válida. Pode-se então querer criar caracterizações mais complexas, em cujo caso o problema pode se tornar com dimensões muito altas para para OLS. \n",
    "* O *SparseLinearDML* pode lidar com essas configurações por meio do uso do Lasso desviado. \n",
    "* O *CausalForestDML* não precisa de caracterização explícita e aprende os modelos CATE não lineares baseados em floresta, automaticamente. \n",
    "\n",
    "Como há não lineariedade, não conseguimos interpretar os coeficientes diretamente. Conseguimos o ATE, e apenas podemos interpretar o efeito do tratamento para um indivíduo com uma característica específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -210.026      26.169 -8.026    0.0      -261.315      -158.736\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "  120.872        -397.661         131.258\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     123.672       -450.408        161.909\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# Definir as variáveis\n",
    "X = df[['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']]\n",
    "D = df['D']\n",
    "y = df['Y']\n",
    "\n",
    "# Converter variáveis categóricas em dummies (se necessário)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Definir os modelos de Machine Learning para (i) X em Y, e (ii) X em D\n",
    "model_y = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "model_d = GradientBoostingClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "# Criar o estimador LinearDML\n",
    "estimator = LinearDML(model_y=model_y,\n",
    "                      model_t=model_d,\n",
    "                      discrete_treatment=True,\n",
    "                      cv=10,\n",
    "                      random_state=123)\n",
    "\n",
    "# Ajustar o modelo\n",
    "estimator.fit(y, D, X=X)\n",
    "\n",
    "ate_inf = estimator.ate_inference(X=X)\n",
    "print(ate_inf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Interpretação:***\n",
    "\n",
    "\n",
    "***Primeiro Quadro:***\n",
    "\n",
    "* *mean_point*: ATE estimado = –210.03g (efeito médio do tabagismo no peso ao nascer)\n",
    "* *stderr_mean*: Erro padrão da média estimada = 26.17\n",
    "* *zstat*: Estatística z = –8.03\n",
    "* *pvalue*: Valor-p = 0.0 → Altamente significativo\n",
    "* *ci_mean_lower*: Limite inferior do IC 95% = –261.32\n",
    "* *ci_mean_upper*: Limite superior do IC 95% = –158.74\n",
    "* Interpretação: o efeito médio do tabagismo reduz o peso do bebê em aproximadamente 210g, com alta precisão estatística.\n",
    "\n",
    "***Segundo Quadro:*** Distribuição empírica dos efeitos individuais (CATEs)\n",
    "\n",
    "* *std_point*: Desvio padrão dos efeitos individuais estimados (CATEs) = 120.87\n",
    "* *pct_point_lower*: Percentil 2.5% dos CATEs = –397.66\n",
    "* *pct_point_upper*: Percentil 97.5% dos CATEs = +131.26 \n",
    "* Interpretação: há heterogeneidade substancial nos efeitos estimados. Para algumas mães, o tabagismo pode reduzir até quase 400g; para outras, o efeito é muito menor (e até positivo, embora improvável).\n",
    "\n",
    "***Terceiro Quadro:***\n",
    "* *stderr_point*: Erro padrão do ATE considerando a variabilidade total dos CATEs = 123.67\n",
    "* *ci_point_lower*: Limite inferior do IC 95% = –450.41\n",
    "* *ci_point_upper*:\tLimite superior do IC 95% = +161.91\n",
    "* Interpretação: esse IC é mais conservador, pois considera a variância entre indivíduos. Ele mostra que, mesmo com heterogeneidade, o ATE segue sendo negativo, mas com mais incerteza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideração Geral do Resultado\n",
    "\n",
    "* O ATE é negativo e significativo, sugerindo que o tabagismo reduz o peso do bebê ao nascer em média em –210g.\n",
    "* Há heterogeneidade dos efeitos individuais (CATEs): nem todas as mães sofrem o mesmo impacto.\n",
    "* A inferência robusta mostra que, mesmo considerando essa heterogeneidade, o resultado se mantém relevante — embora o intervalo seja mais amplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos a estrutura paramétrica do modelo CATE estimado, ou seja, como o efeito do tratamento varia com as covariáveis X (o efeito do tratamento depende linearmente de X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Coefficient Results                       \n",
      "================================================================\n",
      "          point_estimate  stderr zstat  pvalue ci_lower ci_upper\n",
      "----------------------------------------------------------------\n",
      "casada            28.828  55.792  0.517  0.605  -80.522  138.178\n",
      "mage             -11.333   5.261 -2.154  0.031  -21.644   -1.022\n",
      "medu              17.987  11.957  1.504  0.133   -5.449   41.422\n",
      "fhisp             66.089 130.914  0.505  0.614 -190.497  322.676\n",
      "mhisp            -85.452 161.644 -0.529  0.597 -402.269  231.364\n",
      "foreign           348.68 153.471  2.272  0.023   47.882  649.478\n",
      "alcohol          -30.274  91.655  -0.33  0.741 -209.915  149.368\n",
      "deadkids          37.647  51.511  0.731  0.465  -63.312  138.606\n",
      "nprenatal        -14.287   6.922 -2.064  0.039  -27.853   -0.721\n",
      "mrace           -144.841 102.649 -1.411  0.158 -346.029   56.347\n",
      "frace             48.413  99.684  0.486  0.627 -146.964  243.791\n",
      "fage              -0.022   2.781 -0.008  0.994   -5.472    5.427\n",
      "fedu              -0.491   8.399 -0.059  0.953  -16.954   15.971\n",
      "                       CATE Intercept Results                       \n",
      "====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower ci_upper\n",
      "--------------------------------------------------------------------\n",
      "cate_intercept         57.711 166.066 0.348  0.728 -267.772  383.194\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n"
     ]
    }
   ],
   "source": [
    "print(estimator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos interpretar os resultados\n",
    "\n",
    "* *mage* = -11.333\t\n",
    "  * A cada 1 ano a mais de idade materna, o efeito do tabagismo no peso do bebê se torna 11g mais negativo (ceteris paribus).\n",
    "* *nprenatal* = -14.287\t\n",
    "  * A cada consulta pré-natal adicional, o efeito do tabagismo se torna 14g mais negativo.\n",
    "* *foreign* = 348.68\t\n",
    "  * Para mães estrangeiras, o efeito do tabagismo é 348g menos negativo do que para mães não estrangeiras (sugere heterogeneidade positiva).\n",
    "* Demais coeficientes não significativos\n",
    "  * Muitos coeficientes têm p-valor > 0.05 → sem evidência forte de que afetam o CATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate  stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                           \n",
      "0        -256.114  95.479 -2.682   0.007  -443.249    -68.98\n"
     ]
    }
   ],
   "source": [
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Certificar-se de que as colunas correspondem às usadas no modelo\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular o efeito e obter a inferência\n",
    "effect_inf = estimator.effect_inference(X_new)\n",
    "print(effect_inf.summary_frame())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate  stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                           \n",
      "0        -199.602  26.942 -7.408     0.0  -252.408  -146.796\n"
     ]
    }
   ],
   "source": [
    "X_new1 = pd.DataFrame({\n",
    "    'casada': [0.7],\n",
    "    'mage': [26.5],\n",
    "    'medu': [12.7],\n",
    "    'fhisp': [0.04],\n",
    "    'mhisp': [0.03],\n",
    "    'foreign': [0.05],\n",
    "    'alcohol': [0.03],\n",
    "    'deadkids': [0.26],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [0.84],\n",
    "    'frace': [0.81],\n",
    "    'fage': [27.27],\n",
    "    'fedu': [12.31]\n",
    "})\n",
    "\n",
    "# Certificar-se de que as colunas correspondem às usadas no modelo\n",
    "X_new1 = pd.get_dummies(X_new1, drop_first=True)\n",
    "\n",
    "# Calcular o efeito e obter a inferência\n",
    "effect_inf = estimator.effect_inference(X_new1)\n",
    "print(effect_inf.summary_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4642.000000\n",
       "mean       10.758078\n",
       "std         3.681084\n",
       "min         0.000000\n",
       "25%         9.000000\n",
       "50%        11.000000\n",
       "75%        13.000000\n",
       "max        40.000000\n",
       "Name: nprenatal, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['nprenatal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nprenatal  point_estimate   stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "0          0         -56.733   80.025 -0.709   0.478  -213.579   100.113\n",
      "1          9        -185.315   29.369 -6.310   0.000  -242.878  -127.753\n",
      "2         11        -213.889   26.174 -8.172   0.000  -265.189  -162.589\n",
      "3         13        -242.463   29.847 -8.123   0.000  -300.962  -183.963\n",
      "4         40        -628.209  202.930 -3.096   0.002 -1025.945  -230.474\n"
     ]
    }
   ],
   "source": [
    "# Valores médios fixos para todas as covariáveis\n",
    "X_base = {\n",
    "    'casada': [0.7],\n",
    "    'mage': [26.5],\n",
    "    'medu': [12.7],\n",
    "    'fhisp': [0.04],\n",
    "    'mhisp': [0.03],\n",
    "    'foreign': [0.05],\n",
    "    'alcohol': [0.03],\n",
    "    'deadkids': [0.26],\n",
    "    'nprenatal': [None],  # será substituído\n",
    "    'mrace': [0.84],\n",
    "    'frace': [0.81],\n",
    "    'fage': [27.27],\n",
    "    'fedu': [12.31]\n",
    "}\n",
    "\n",
    "# Valores específicos de nprenatal a testar\n",
    "nprenatal_vals = [0, 9, 11, 13, 40]\n",
    "\n",
    "# Lista para armazenar resultados\n",
    "resultados = []\n",
    "\n",
    "for val in nprenatal_vals:\n",
    "    # Atualiza o valor de nprenatal\n",
    "    X_base['nprenatal'] = [val]\n",
    "    \n",
    "    # Cria o DataFrame com esse único perfil\n",
    "    X_tmp = pd.DataFrame(X_base)\n",
    "    X_tmp = pd.get_dummies(X_tmp, drop_first=True)\n",
    "    \n",
    "    # Estima o efeito e obtém inferência\n",
    "    effect_inf = estimator.effect_inference(X_tmp).summary_frame()\n",
    "    effect_inf['nprenatal'] = val\n",
    "    resultados.append(effect_inf)\n",
    "\n",
    "# Concatena todos os resultados\n",
    "tabela_final = pd.concat(resultados).reset_index(drop=True)\n",
    "\n",
    "# Exibe tabela formatada\n",
    "print(tabela_final[['nprenatal', 'point_estimate', 'stderr', 'zstat', 'pvalue', 'ci_lower', 'ci_upper']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nprenatal  point_estimate    stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                                        \n",
      "0          0        1223.016  2672.433  0.458   0.647 -4014.856  6460.888\n",
      "1          9        1218.594  2648.752  0.460   0.645 -3972.866  6410.053\n",
      "2         11        1217.611  2643.755  0.461   0.645 -3964.054  6399.275\n",
      "3         13        1216.628  2638.855  0.461   0.645 -3955.433  6388.689\n",
      "4         40        1203.361  2582.508  0.466   0.641 -3858.261  6264.983\n"
     ]
    }
   ],
   "source": [
    "# Valores médios das covariáveis\n",
    "X_base = {\n",
    "    'casada': 0.7,\n",
    "    'mage': 26.5,\n",
    "    'medu': 12.7,\n",
    "    'fhisp': 0.04,\n",
    "    'mhisp': 0.03,\n",
    "    'foreign': 0.05,\n",
    "    'alcohol': 0.03,\n",
    "    'deadkids': 0.26,\n",
    "    'mrace': 0.84,\n",
    "    'frace': 0.81,\n",
    "    'fage': 27.27,\n",
    "    'fedu': 12.31\n",
    "}\n",
    "\n",
    "# Lista dos valores de nprenatal a testar\n",
    "nprenatal_vals = [0, 9, 11, 13, 40]\n",
    "\n",
    "# Lista para armazenar os DataFrames e valores\n",
    "X_list = []\n",
    "\n",
    "for val in nprenatal_vals:\n",
    "    X_tmp = X_base.copy()\n",
    "    X_tmp['nprenatal'] = val\n",
    "    X_list.append(X_tmp)\n",
    "\n",
    "# Criar DataFrame final\n",
    "X_test = pd.DataFrame(X_list)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Estimar efeitos com inferência\n",
    "effect_inf = estimator.effect_inference(X_test)\n",
    "result_df = effect_inf.summary_frame()\n",
    "result_df.insert(0, 'nprenatal', nprenatal_vals)\n",
    "\n",
    "# Exibir tabela\n",
    "print(result_df[['nprenatal', 'point_estimate', 'stderr', 'zstat', 'pvalue', 'ci_lower', 'ci_upper']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do NonParamDML, não é possível obter uma tabela semelhante com coeficientes para cada covariável. Isso se deve à natureza não paramétrica do estimador, que não assume uma forma funcional específica para a relação entre as covariáveis e o efeito do tratamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -214.954     186.112 -1.155  0.248      -579.728       149.819\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "  143.065         -494.49          58.236\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     234.746       -702.526        231.413\n",
      "------------------------------------------\n",
      "\n",
      "Note: The stderr_mean is a conservative upper bound.\n"
     ]
    }
   ],
   "source": [
    "from econml.dml import CausalForestDML\n",
    "from econml.dml import NonParamDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Criar o estimador ForestDML\n",
    "estimator_Forest = CausalForestDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, random_state=123),\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Ajustar o modelo\n",
    "estimator_Forest.fit(y, D, X=X)\n",
    "\n",
    "# Obter a inferência do ATE\n",
    "ate_inf_forest = estimator_Forest.ate_inference(X=X)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(ate_inf_forest.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population summary results are available only if `cache_values=True` at fit time!\n",
      "       Doubly Robust ATE on Training Data Results       \n",
      "========================================================\n",
      "    point_estimate stderr zstat pvalue ci_lower ci_upper\n",
      "--------------------------------------------------------\n",
      "ATE        -120.52 94.142 -1.28    0.2 -305.035   63.995\n",
      "     Doubly Robust ATT(T=0) on Training Data Results      \n",
      "==========================================================\n",
      "    point_estimate stderr  zstat  pvalue ci_lower ci_upper\n",
      "----------------------------------------------------------\n",
      "ATT       -232.151 13.476 -17.227    0.0 -258.564 -205.738\n",
      "     Doubly Robust ATT(T=1) on Training Data Results     \n",
      "=========================================================\n",
      "    point_estimate  stderr zstat pvalue ci_lower ci_upper\n",
      "---------------------------------------------------------\n",
      "ATT        367.605 502.013 0.732  0.464 -616.323 1351.533\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(estimator_Forest.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate  stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                           \n",
      "0        -192.011  94.156 -2.039   0.041  -376.554    -7.468\n"
     ]
    }
   ],
   "source": [
    "# Definir as características específicas\n",
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Converter variáveis categóricas em dummies, se aplicável\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular a estimativa pontual do efeito\n",
    "cate_new = estimator_Forest.effect(X_new)\n",
    "\n",
    "# Obter a inferência estatística\n",
    "effect_inf = estimator_Forest.effect_inference(X_new)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(effect_inf.summary_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como podemos avaliar o desempenho do modelo CATE?**\n",
    "\n",
    "Cada uma das classes DML tem um atributo score_ depois de serem ajustadas. Então, é possível acessar esse atributo e comparar o desempenho em diferentes parâmetros de modelagem (quanto menor a pontuação, melhor):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerações Finais\n",
    "\n",
    "Vejamos um resumo do que foi visto.\n",
    "\n",
    "* O principal objetivo do DML é ajustar e remover a variável de confusão de forma que a variável de interesse (tratamento) e o resultado fiquem \"ortogonais\" ou \"independentes\".\n",
    "* DML combina métodos de aprendizado de máquina com técnicas econométricas para estimar efeitos causais.\n",
    "* A técnica geralmente envolve a aplicação de aprendizado de máquina para prever tanto o tratamento quanto o resultado usando as variáveis observáveis de confusão, e então os resíduos dessas previsões são utilizados em um segundo estágio para estimar o efeito causal.\n",
    "  * Primeiro Estágio: Aplicar modelos de aprendizado de máquina para prever a variável de tratamento e o resultado.\n",
    "  * Segundo Estágio: Utilizar os resíduos dessas previsões em um modelo de regressão para estimar o efeito causal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
