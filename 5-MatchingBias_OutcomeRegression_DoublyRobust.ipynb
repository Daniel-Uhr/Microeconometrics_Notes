{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching with Bias Correction, Outcome Regression, and Doubly Robust Estimation\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* Matching Estimator with Bias Corretion - Abadie e Imbens (2011)\n",
    "  * Aplicação no Python\n",
    "* Outcome Regression\n",
    "  * Aplicação no Python\n",
    "* Doubly Robust Estimation\n",
    "  * Aplicação no Python\n",
    "* Boas Práticas\n",
    "\n",
    "\n",
    "## Referências\n",
    "\n",
    "* Abadie e Imbens (2011). Bias-Corrected Matching Estimators for Average Treatment Effects. Journal of Business & Economic Statistics, 29(1), 1-11.\n",
    "* Heckman, James, Hidehiko Ichimura, Jefrey Smith, and Petra Todd. (1998). Characterizing selection bias using experimental data\". Econometrica 66.5, pp. 1017-1098.\n",
    "* Cunningham, S. W. (2013). Causal inference: The mixtape. https://www.scunning.com/mixtape.html\n",
    "* Courthoud, Matteo. Understanding AIPW. https://matteocourthoud.github.io/post/aipw/ \n",
    "* Matheus Facure. Doubly Robust Estimation. https://matheusfacure.github.io/python-causality-handbook/12-Doubly-Robust-Estimation.html\n",
    "* Pearl, J. (2000). Causality: Models, reasoning, and inference. Cambridge University Press.\n",
    "* Joshua D. Angrist and Jörn-Steffen Pischke (2009). Mostly Harmless Econometrics: An Empiricist's Companion. Princeton University Press.\n",
    "* Morgan, S. L., & Winship, C. (2014). Counterfactuals and causal inference: Methods and principles for social research. Cambridge University Press. Capítulo 3.\n",
    "\n",
    "**Pacotes Python**\n",
    "* pyDRReg - Regressão Duplamente Robusta, Outcome Regression e Inverse Probability Weighting, com erro padrão bootstrap.\n",
    "* Instalação: `pip install git+https://github.com/Daniel-Uhr/pyDRReg.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Estimator with Bias Corretion\n",
    "\n",
    "**Abadie e Imbens (2011)** introduziram técnicas de correção de viés com estimadores de correspondência **quando há discrepâncias de correspondência em amostras finitas**. A correspondência é tendenciosa devido a essas discrepâncias de correspondência deficientes. Então, vamos entender a sugestão dos autores.\n",
    "\n",
    "$$ \\widehat{\\delta_{ATT}} = \\frac{1}{N_{T}} \\sum_{D_{i}=1} (Y_{i}-Y_{j(i)}) $$\n",
    "\n",
    "onde cada $i$ e $j(i)$ unidades são combinadas, e $X_{i} \\approx X_{j(i)}$ e $D_{j(i)}=0$. \n",
    "\n",
    "Definimos os resultados da expectativa condicional\n",
    "\n",
    "$$ \\mu^{0}(x) = E[Y | X=x, D=0] = E[Y^{0}|X=x] $$\n",
    "$$ \\mu^{1}(x) = E[Y | X=x, D=1] = E[Y^{1}|X=x] $$\n",
    "\n",
    "Observe que essas são apenas as funções de resultado condicional esperadas com base na equação de comutação para os grupos de controle e de tratamento. Como sempre, escrevemos o valor observado em função dos resultados condicionais esperados e de algum elemento estocástico:\n",
    "\n",
    "$$ Y_{i} = \\mu^{D_{i}}(X_{i}) + \\epsilon_{i} $$\n",
    "\n",
    "\n",
    "Reescrevendo o estimador ATT usando o $\\mu$ acima:\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT} = \\frac{1}{N_{T}} \\sum_{D_{i}=1} ( \\mu^{1}(X_{i}) + \\epsilon_{i}) - (\\mu^{0}(X_{j(i)}) + \\epsilon_{j(i)}) = \\frac{1}{N_{T}} \\sum_{D_{i}=1}( \\mu^{1}(X_{i}) -  \\mu^{0}(X_{j(i)})) + \\frac{1}{N_{T}} \\sum_{D_{i}=1}(\\epsilon_{i} - \\epsilon_{j(i)}) $$\n",
    "\n",
    "Observe que a primeira igualdade é apenas o ATT com o elemento estocástico incluído na linha anterior. E a segunda igualdade reorganiza-a de modo a obtermos dois termos: o ATT estimado mais a diferença média nos termos estocásticos para a amostra correspondente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparando o estimador $\\widehat{\\delta_{ATT}}$ com o verdadeiro $\\delta_{ATT}$, Abadie e Imbens (2011) propuseram um estimador de correspondência com viés de correção (Bias Correction) para as diferenças das características observaveis ($X$) dado por:\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{BC} = \\frac{1}{N_{T}} \\sum_{D_{i}=1} [(Y_{i} - Y_{j(i)}) - (\\hat{\\mu}^{0}(X_{i}) - \\hat{\\mu}^{0}(X_{j(i)}))] $$\n",
    "\n",
    "onde o $\\hat{\\mu}^{0}(X)$ é uma estimativa OLS de $E[Y|X=x, D=0]$. Vejamos o exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tabela 5.8: Exemplo de correspondência (para ilustrar a correção de viés)\n",
    "\n",
    "| Unidade |   $Y^{1}$     |  $Y^{0}$     | $D$      |  $X $    |\n",
    "|---------|--------|-------|--------|-------|\n",
    "| 1       | 5      |       | 1      | 11    |\n",
    "| 2       | 2      |       | 1      | 7     |\n",
    "| 3       | 10     |       | 1      | 5     |\n",
    "| 4       | 6      |       | 1      | 3     |\n",
    "| 5       |        | 4     | 0      | 10    |\n",
    "| 6       |        | 0     | 0      | 8     |\n",
    "| 7       |        | 5     | 0      | 4     |\n",
    "| 8       |        | 1     | 0      |  1    |\n",
    "\n",
    "Observe que não podemos implementar a correspondência exata porque nenhuma das unidades do grupo de tratamento tem uma correspondência exata no grupo de controle. Em vez disso, usamos a correspondência do vizinho mais próximo, que simplesmente fará a correspondência de cada unidade de tratamento com a unidade do grupo de controle cujo valor da covariável é mais próximo daquele da própria unidade do grupo de tratamento. Mas, quando fazemos este tipo de correspondência, criamos necessariamente discrepâncias de correspondência , o que é simplesmente outra forma de dizer que as covariáveis ​​não são perfeitamente combinadas para todas as unidades. \n",
    "\n",
    "$$ \\hat{\\delta}_{ATT} = \\frac{5-4}{4} + \\frac{2-0}{4} + \\frac{10-5}{4} + \\frac{6-1}{4} = 3.25 $$\n",
    "\n",
    "No entanto, o “algoritmo” do vizinho mais próximo cria a Tabela :\n",
    "\n",
    "| Unidade |   $Y^{1}$     |  $Y^{0}$     | $D$      |  $X $    |\n",
    "|---------|--------|-------|--------|-------|\n",
    "| 1       | 5      | **4**      | 1      | 11    |\n",
    "| 2       | 2      | **0**      | 1      | 7     |\n",
    "| 3       | 10     | **5**      | 1      | 5     |\n",
    "| 4       | 6      | **1**      | 1      | 3     |\n",
    "| 5       |        | 4     | 0      | 10    |\n",
    "| 6       |        | 0     | 0      | 8     |\n",
    "| 7       |        | 5     | 0      | 4     |\n",
    "| 8       |        | 1     | 0      |  1    |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação no Python\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se regredimos $X$ em $Y$, obtemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.015\n",
      "Model:                            OLS   Adj. R-squared:                 -0.478\n",
      "Method:                 Least Squares   F-statistic:                   0.03001\n",
      "Date:                Thu, 05 Sep 2024   Prob (F-statistic):              0.878\n",
      "Time:                        14:34:04   Log-Likelihood:                -8.5398\n",
      "No. Observations:                   4   AIC:                             21.08\n",
      "Df Residuals:                       2   BIC:                             19.85\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      2.0872      2.788      0.749      0.532      -9.909      14.083\n",
      "X              0.0718      0.414      0.173      0.878      -1.712       1.855\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   3.412\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.402\n",
      "Skew:                          -0.018   Prob(JB):                        0.818\n",
      "Kurtosis:                       1.448   Cond. No.                         13.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit</th>\n",
       "      <th>Y</th>\n",
       "      <th>D</th>\n",
       "      <th>X</th>\n",
       "      <th>mu0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2.876923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.589744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.446154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.302564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2.805128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2.661538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.374359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.158974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unit   Y  D   X       mu0\n",
       "0     1   5  1  11  2.876923\n",
       "1     2   2  1   7  2.589744\n",
       "2     3  10  1   5  2.446154\n",
       "3     4   6  1   3  2.302564\n",
       "4     5   4  0  10  2.805128\n",
       "5     6   0  0   8  2.661538\n",
       "6     7   5  0   4  2.374359\n",
       "7     8   1  0   1  2.158974"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf \n",
    "\n",
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/scunning1975/mixtape/raw/master/training_bias_reduction.dta\")\n",
    "\n",
    "reg = smf.ols('Y ~ X', data=df[df['D'] == 0]).fit()\n",
    "print(reg.summary())\n",
    "\n",
    "df['mu0']= reg.predict(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou seja, \n",
    "\n",
    "$$ \\hat{\\mu^{0}}(X) = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}X = 2.0872 - 0.0718X $$\n",
    "\n",
    "\n",
    "| Unidade |   $Y^{1}$     |  $Y^{0}$     | $D$      |  $X $    | $\\hat{\\mu}^{0}(X)$ |\n",
    "|---------|--------|-------|--------|-------|---------|\n",
    "| 1       | 5      | 4      | 1      | 11    | 2.88     |\n",
    "| 2       | 2      | 0      | 1      | 7     |  2.59    |\n",
    "| 3       | 10     | 5      | 1      | 5     |  2.45    |\n",
    "| 4       | 6      | 1      | 1      | 3     |  2.30    |\n",
    "| 5       |        | **4**     | 0      | 10    |  **2.81**    |\n",
    "| 6       |        | **0**     | 0      | 8     |  **2.66**  |\n",
    "| 7       |        | **5**     | 0      | 4     |  **2.37**    |\n",
    "| 8       |        | **1**     | 0      |  1    |  **2.16**    |\n",
    "\n",
    "\n",
    "Agora, podemos calcular o estimador de correspondência com correção de viés (Bias Correction - BC):\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{BC} = \\frac{ (5-4) - (2.88 - 2.81)}{4} + \\frac{ (2-0) - (2.59 - 2.66)}{4} + \\frac{ (10-5) - (2.45 - 2.37)}{4} + \\frac{ (6-1) - (2.30 - 2.16)}{4} = 3.195 $$\n",
    "\n",
    "\n",
    "O valor encontrado de aproximadamente **3.20**, é ligeiramente inferior ao ATT não ajustado de **3,25**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome Regression\n",
    "\n",
    "O estimador de \"Outcome Regression\" também é conhecido como Regression Adjustment ou Augmented Regression. O estimador procura utilizar a ideia de resultados potenciais para estimar o efeito causal médio do tratamento. Basicamente, utiliza-se a regressão linear para modelar os grupos de tratamento e controle, e, a partir disso, estimar o efeito médio do tratamento. Vejamos como isso é feito.\n",
    "\n",
    "Lembre-se que o modelo de regressão linear é dado por:\n",
    "\n",
    "$$ Y = \\alpha + \\beta D + \\gamma X + \\epsilon $$\n",
    "\n",
    "Considere $ Y $ como a variável dependente, $ D $ é a variável indicadora de tratamento (1 se o tratamento foi aplicado, 0 caso contrário), $ X $ são as covariáveis observáveis, e $ \\epsilon $ é o termo de erro.\n",
    "\n",
    "Se aplicamos a expectativa condicional em $D$, temos:\n",
    "   \n",
    "$$ E[Y|X, D=1] = \\mu_{1}(X) + E[u_1|X, D=1]  $$\n",
    "\n",
    "$$ E[Y|X, D=0] = \\mu_{0}(X) + E[u_0|X, D=1]  $$\n",
    "\n",
    "Então, no contexto de resultados potenciais, $\\mu_{1}(X) $ e $ \\mu_{0}(X) $ representam as **médias dos resultados potenciais** para os tratados e não tratados, respectivamente, dados $ X $. Os termos $ u_1 $ e $u_0 $ são termos de erro associados aos grupos de tratamento e controle, respectivamente.\n",
    "\n",
    "\n",
    "Ao subtrair essas duas equações, obtemos:\n",
    "\n",
    "$$ E[Y|X, D=1] - E[Y|X, D=0] = \\mu_{1}(X) - \\mu_{0}(X) + [E[u_1|X, D=1] - E[u_0|X, D=1] ] $$\n",
    "\n",
    "\n",
    "No entanto, é importante notar que $ u_1 $ e $ u_0 $ são termos de erro esperados condicionados às covariáveis e ao status de tratamento. Por definição, temos:\n",
    "\n",
    "   $$ E[u_1|X, D=1] = E[Y|X, D=1] - \\mu_{1}(X) $$\n",
    "\n",
    "   $$ E[u_0|X, D=0] = E[Y|X, D=0] - \\mu_{0}(X) $$\n",
    "\n",
    "Isso implica que $ u_1 $ e $ u_0 $ são **os desvios entre as expectativas condicionais observadas e as médias dos resultados potenciais**.\n",
    "\n",
    "\n",
    "É possivel reescrever a diferença entre as médias condicionais como:\n",
    "\n",
    "$$ E[Y|X, D=1] - E[Y|X, D=0] = \\mu_{1}(X) - \\mu_{0}(X) + [E[Y|X, D=1] - \\mu_{1}(X)] - [E[Y|X, D=0] - \\mu_{0}(X)] $$\n",
    "\n",
    "\n",
    "**Outcome Regression - ATE**\n",
    "\n",
    "Simplificando a notação, com $ Y_1 $ representando $ E[Y|X, D=1] $ e $ Y_0 $ representando $ E[Y|X, D=0] $, tomando a esperaça, temos:\n",
    "\n",
    "$$ \\hat{\\beta}^{OR}_{ATE} = E[\\mu_{1}(X) - \\mu_{0}(X)] + E[(Y_{1} - \\mu_{1}(X)) - (Y_{0} - \\mu_{0}(X))] $$\n",
    "\n",
    "Logo,  o termo $E[\\mu_{1}(X) - \\mu_{0}(X)] $ representa o efeito causal médio do tratamento ajustado pelas covariáveis $ X $. O segundo termo, $ E[(Y_{1} - \\mu_{1}(X)) - (Y_{0} - \\mu_{0}(X))] $, deveria, em teoria, ser zero se os erros $ u_1 $ e $ u_0 $ tiverem expectativa zero. Em um experimento controlado, esperaríamos que $E[Y_{1} - \\mu_{1}(X)] = 0$ e $E[Y_{0} - \\mu_{0}(X)] = 0$, o que simplificaria a expressão para o efeito causal médio ajustado pelas covariáveis. \n",
    "\n",
    "Entretanto, desvios sistemáticos entre os grupos de tratados e controles podem ocorrer, principalmente, devido ao **viés de seleção** (Covariáveis podem estar desbalanceadas entre os grupos de tratamento e controle), ou, se houver variáveis não observadas que afetam os resultados. \n",
    "\n",
    "Portanto, em caso de um experimento, a expressão para o ATE, ajustado pelas covariáveis, deve simplificar para $ \\hat{\\beta}^{OR} = E[\\mu_{1}(X) - \\mu_{0}(X)] $. Isso mostra que a regressão linear pode ser usada para estimar o ATE, desde que o modelo seja corretamente especificado e os pressupostos de linearidade e ausência de viés dos erros sejam válidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome Regression - ATT**\n",
    "\n",
    "Para obter o efeito médio do tratamento para os tratados, precisamos construir a ideia contrafactual. Ou seja, utilizamos a projeção do grupo de controle com as características X do grupo de tratamento para construir o contrafactual. Dessa forma a estimativa do efeito médio do tratamento para os tratados (ATT) é dada por:\n",
    "\n",
    "\n",
    "   $$ \\hat{\\beta}^{OR}_{ATT} = E[\\mu_{1}(X^{1}) - \\mu_{0}(X^{1})] + E[(Y_{1} - \\mu_{1}(X^{1})) - (Y_{0} - \\mu_{0}(X^{1}))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação no Python\t\n",
    "\n",
    "Vamos fazer um exemplo prático no Python, com os dados das mães fumantes e não fumantes durante a gestação, e o peso dos bebês ao nascer. \n",
    "\n",
    "Primeiramente farei os códigos na \"mão\", e depois utilizarei o pacote pyDRReg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "\n",
    "# Criar a variável de resultado\n",
    "df['Y'] = df['bweight']\n",
    "\n",
    "# Crie a variável 'Treated' com valor inicial de 0\n",
    "df['Treated'] = 0\n",
    "# Recodifique 'Treated' para 1 se 'mbsmoke' for igual a 'smoker'\n",
    "df.loc[df['mbsmoke'] == 'smoker', 'Treated'] = 1\n",
    "\n",
    "# Criar variáveis de controle\n",
    "df['Mmarried'] = 0\n",
    "df.loc[df['mmarried'] == 'married', 'Mmarried'] = 1\n",
    "df['casada'] = 0\n",
    "df.loc[df['mmarried']=='married', 'casada'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após carregar os dados vamos tentar aplicar o Outcome Regression para estimar o ATE.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome Regression - ATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Regression Estimate - ATE (OR - ATE): -236.52788533678506\n"
     ]
    }
   ],
   "source": [
    "def OR_ate(df, X_cols, T_col, Y_col):\n",
    "    X_np = df[X_cols].values  # Converter X para numpy array\n",
    "    T_np = df[T_col].values  # Converter T para numpy array\n",
    "    Y_np = df[Y_col].values  # Converter Y para numpy array\n",
    "    # Essas linhas convertem as colunas de interesse do DataFrame (df) em arrays numpy, o que facilita a manipulação e os cálculos subsequentes.\n",
    "        \n",
    "    # Ajustar regressão linear para não tratados (D=0)\n",
    "    model_0 = LinearRegression().fit(X_np[T_np==0], Y_np[T_np==0])\n",
    "    mu0 = model_0.predict(X_np)\n",
    "    # Aqui, ajusta-se um modelo de regressão linear para as observações que não receberam o tratamento (T_np == 0). Em seguida, são feitas previsões (mu0) para todas as observações com base neste modelo.\n",
    "    \n",
    "    # Ajustar regressão linear para tratados (D=1)\n",
    "    model_1 = LinearRegression().fit(X_np[T_np==1], Y_np[T_np==1])\n",
    "    mu1 = model_1.predict(X_np)\n",
    "    # Da mesma forma, ajusta-se um modelo de regressão linear para as observações que receberam o tratamento (T_np == 1) e são feitas previsões (mu1) para todas as observações com base neste modelo.\n",
    "    \n",
    "    # Calcular o efeito causal médio\n",
    "    effect = np.mean(mu1 - mu0)\n",
    "    # Calcula-se a diferença média entre as previsões dos modelos para tratados e não tratados, resultando em uma estimativa preliminar do efeito causal médio\n",
    "    \n",
    "    # Calcular os desvios para todas as observações\n",
    "    deviations = (Y_np - (T_np * mu1 + (1 - T_np) * mu0))\n",
    "    # Os desvios são calculados como a diferença entre os valores observados de Y e as previsões ponderadas dos modelos de tratados e não tratados.\n",
    "    \n",
    "    # Calcular a média dos desvios\n",
    "    deviations_mean = np.mean(deviations)\n",
    "    # Calcula-se a média dos desvios, que é usada para ajustar a estimativa do efeito causal médio.\n",
    "    \n",
    "    # Calcular o OR\n",
    "    OR_ate_estimate = effect + deviations_mean\n",
    "    \n",
    "    return OR_ate_estimate\n",
    "\n",
    "# Definir as colunas de interesse\n",
    "T_col = 'Treated'\n",
    "Y_col = 'Y'\n",
    "X_cols = ['casada', 'mage', 'medu']\n",
    "\n",
    "# Calcular o OR\n",
    "result_OR_ate = OR_ate(df, X_cols, T_col, Y_col)\n",
    "print(\"Outcome Regression Estimate - ATE (OR - ATE):\", result_OR_ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado da estimativa de ATE (Average Treatment Effect) usando a técnica de Outcome Regression é de aproximadamente -236.53. Isso significa que, em média, o efeito do fumo durante a gestação sobre o resultado $Y$ (peso dos bebês ao nascer) é uma redução de aproximadamente 236.53 gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Regression (ATT): -215.24538690027902\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def OR_att(df, X_cols, T_col, Y_col):\n",
    "    # Separar dados tratados (D=1) e não tratados (D=0)\n",
    "    X_treated = df[df[T_col] == 1][X_cols]\n",
    "    Y_treated = df[df[T_col] == 1][Y_col]\n",
    "    X_control = df[df[T_col] == 0][X_cols]\n",
    "    Y_control = df[df[T_col] == 0][Y_col]\n",
    "\n",
    "    # Ajustar modelos de regressão linear\n",
    "    model_treated = LinearRegression().fit(X_treated, Y_treated)\n",
    "    model_control = LinearRegression().fit(X_control, Y_control)\n",
    "\n",
    "    # Calcular previsões para tratados e não tratados\n",
    "    mu1_X = model_treated.predict(X_treated)\n",
    "    mu0_X = model_control.predict(X_treated)  # Usando X_treated para manter o contrafactual consistente\n",
    "\n",
    "    # Calcular desvios para todas as observações\n",
    "    deviations_treated = Y_treated - mu1_X\n",
    "    deviations_control = Y_control - model_control.predict(X_control)\n",
    "\n",
    "    # Calcular a média dos desvios tratados e não tratados\n",
    "    deviations_mean_treated = deviations_treated.mean()\n",
    "    deviations_mean_control = deviations_control.mean()\n",
    "\n",
    "    # Calcular a média geral dos desvios\n",
    "    deviations_mean = deviations_treated.mean()  # Considerando apenas os tratados para o ATT\n",
    "\n",
    "    # Calcular ATT\n",
    "    OR_att_estimate = (mu1_X.mean() - mu0_X.mean()) + deviations_mean\n",
    "\n",
    "    return OR_att_estimate\n",
    "\n",
    "T_col = 'Treated'\n",
    "Y_col = 'Y'\n",
    "X_cols = ['casada', 'mage', 'medu']\n",
    "\n",
    "result_OR_att = OR_att(df, X_cols, T_col, Y_col)\n",
    "print(\"Outcome Regression (ATT):\", result_OR_att)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo a análise de forma mais fácil, pelo pacote pyDRReg, encontramos os mesmos resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome Regression - ATT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator          OR\n",
      "1        Method         ATT\n",
      "2      Estimate -215.245387\n",
      "3  bootstrap_SE   22.410568\n",
      "4        t-stat   -9.604638\n",
      "5       p-value         0.0\n",
      "6      CI Lower   -259.1701\n",
      "7      CI Upper -171.320674\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "OR_att = pyDRReg(df, X_vars, T_var, Y_var, method='att', estimator='OR', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(OR_att.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome Regression - ATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator          OR\n",
      "1        Method         ATE\n",
      "2      Estimate -236.527885\n",
      "3  bootstrap_SE   24.192087\n",
      "4        t-stat   -9.777077\n",
      "5       p-value         0.0\n",
      "6      CI Lower -283.944375\n",
      "7      CI Upper -189.111396\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "OR_ate = pyDRReg(df, X_vars, T_var, Y_var, method='ate', estimator='OR', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(OR_ate.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado da estimativa de ATT (Average Treatment Effect on the Treated) usando a técnica de Outcome Regression é de aproximadamente -215.25; Isso significa que, em média, o efeito do fumo durante a gestação sobre o resultado $Y$ (peso dos bebês ao nascer) é uma redução de aproximadamente 215.25 gramas para os tratados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doubly Robust Estimation (DR)\n",
    "\n",
    "A estimativa duplamente robusta combina uma forma de \"Outcome Regression\" com um modelo de ponderação (ou seja, utilizando o escore de propensão) para estimar o efeito causal sobre um resultado. Quando usados individualmente para estimar um efeito causal, os métodos de regressão de resultados e escore de propensão são não enviesados apenas se o modelo estatístico for especificado corretamente. O **estimador duplamente robusto** combina estas 2 abordagens de modo que apenas 1 dos 2 modelos precisa ser especificado corretamente para obter um estimador de efeito não-viesado.\n",
    "\n",
    "A especificação correta do modelo de regressão é um pressuposto fundamental na análise econométrica. Quando o objetivo é ajustar o fator de confusão, o estimador é consistente (e, portanto, assintoticamente não-enviesado) se o modelo refletir as verdadeiras relações entre a exposição e os fatores de confusão com o resultado. Na prática, nunca poderemos saber se algum modelo específico representa com precisão essas relações. **A estimativa duplamente robusta combina regressão de resultados com ponderação pelo escore de propensão (PS), de modo que o estimador seja robusto à especificação incorreta de um (mas não de ambos) desses modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome Regression Approach**\n",
    "\n",
    "Vimos que:\n",
    "\n",
    "$$ \\hat{\\beta}_{ATE}^{OR} = E[\\mu_{1}(X) - \\mu_{0}(X)] + E[(Y_{1} - \\mu_{1}(X)) - (Y_{0} - \\mu_{0}(X))] $$\n",
    "\n",
    "e\n",
    "\n",
    "$$ \\hat{\\beta}^{OR}_{ATT} = E[\\mu_{1}(X^{1}) - \\mu_{0}(X^{1})] + E[(Y_{1} - \\mu_{1}(X^{1})) - (Y_{0} - \\mu_{0}(X^{1}))] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "\n",
    "**Approach de Inverse Probability Weighting (IPW)**\n",
    "\n",
    "Nesta abordagem, o viés de confusão é ajustado por meio de técnicas de matching (pareamento) e ponderação pelo escore de propensão (Peso = $W$). As ponderações são calculadas da seguinte forma:\n",
    "\n",
    "$$ W_{ATE} = \\frac{D}{\\hat{p}(X)} + \\frac{1-D}{1-\\hat{p}(X)} $$\n",
    "\n",
    "e,\n",
    "\n",
    "$$ W_{ATT} = D + (1-D)\\frac{\\hat{p}(X)}{1-\\hat{p}(X)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach do Doubly Robust Estimation (DR)**\n",
    "\n",
    "A abordagem Doubly Robust Estimation (DR) combina as vantagens das abordagens de Outcome Regression e de Ponderação pela Probabilidade Inversa. Isso proporciona uma maior robustez aos resultados. Os estimadores duplamente robustos para o Average Treatment Effect (ATE) e o Average Treatment Effect on the Treated (ATT) são dados pelas seguintes fórmulas:\n",
    "\n",
    "* **Doubly Robust Estimation for Average Treatment Effect (ATE)**\n",
    "\n",
    "$$ \\hat{\\beta_{ATE}^{DR}} =  \\mathbb{E} \\left[ (\\mu_1(X) - \\mu_0 (X)) \\right] + \\mathbb{E} \\left[ \\frac{D}{\\hat{p}(X)}.(Y_{1} - \\mu_1 (X)) - \\frac{(1-D)}{1-\\hat{p}(X)}.(Y_{0} - \\mu_0 (X)) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "* **Doubly Robust Estimation for Average Treatment Effect on the Treated (ATT)**\n",
    "\n",
    "$$ \\hat{\\beta_{ATT}^{DR}} = \\mathbb{E} \\left[ (\\mu_1 (X) - \\mu_0 (X)) \\right] + \\mathbb{E} \\left[ D(Y - \\mu_1 (X)) - \\frac{(1-D)\\hat{p}(X)}{1-\\hat{p}(X)}.(Y - \\mu_0 (X)) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que realizamos um \"ajuste\" nos resíduos da regressão de resultados, ponderando-os pelo escore de propensão. Isso garante que o estimador seja robusto à especificação incorreta de um dos modelos. O \"ajuste\" é essencialmente um estimador IPW realizado sobre os resíduos.\n",
    "\n",
    "Por isso que o Doubly Robust Estimation também é conhecido como Augmented Inverse Probability Weighting (AIPW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por que a o estimador Duplamente Robusto (*Augmented Inverse Probability Weighting* - AIPW) é tão atraente?**\n",
    "\n",
    "A razão é que só precisamos de uma das duas previsões, *$\\hat{\\mu}$* ou *$\\hat{p}$*, para que a estimativa seja correta (não enviesada/imparcial). \n",
    "* Se ambos os modelos estiverem corretos, o estimador será mais eficiente do que qualquer um dos modelos sozinho. \n",
    "* Se um dos modelos estiver errado, o estimador ainda será consistente, desde que o outro modelo esteja correto. \n",
    "\n",
    "Isso é uma grande vantagem em relação a outras abordagens, como a regressão de resultados ou a ponderação pelo escore de propensão, que exigem que ambos os modelos estejam corretos para que o estimador seja consistente.\n",
    "\n",
    "Suponha que $\\hat{\\mu}$ esteja especificado corretamente. Então $E[\\hat{\\mu}^{d}(x)=E[Y|X=x, D=d]$ , então o estimador DR é consistente, mesmo que o modelo de propensão $\\hat{p}$ esteja mal especificado.\n",
    "\n",
    "$$ \\hat{\\beta^{DR}} =  \\mathbb{E} \\left[ (\\mu_1(X) - \\mu_0 (X)) +  \\frac{D}{\\hat{p}(X)}.(Y_{1} - \\mu_1 (X)) - \\frac{(1-D)}{1-\\hat{p}(X)}.(Y_{0} - \\mu_0 (X)) \\right] = $$\n",
    "\n",
    "$$  =  \\mathbb{E} \\left[ (\\mu_1(X) - \\mu_0 (X)) \\right] = $$\n",
    "\n",
    "$$  =  \\mathbb{E} \\left[ Y^{1} - Y^{0} \\right] = $$\n",
    "\n",
    "$$ = \\beta $$\n",
    "\n",
    "A intuição é que, se $\\hat{\\mu}$ está especificado corretamente é imparcial e o fator de ajuste desaparece, uma vez que os resíduos convergem para zero.\n",
    "\n",
    "\n",
    "Por outro lado, suponha $\\hat{p}$ está especificado corretamente, ou seja, $E[\\hat{p}(X)]=P(D=1|X)$, então o estimador DR é consistente, mesmo que o modelo de resultados $\\hat{\\mu}$ esteja mal especificado.\n",
    "\n",
    "$$ \\hat{\\beta^{DR}} =  \\mathbb{E} \\left[ (\\mu_1(X) - \\mu_0 (X)) +  \\frac{D}{\\hat{p}(X)}.(Y_{1} - \\mu_1 (X)) - \\frac{(1-D)}{1-\\hat{p}(X)}.(Y_{0} - \\mu_0 (X)) \\right] = $$\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\mu_1(X) - \\mu_0 (X) + \\frac{D}{\\hat{p}(X)}.Y_{1} - \\frac{D}{\\hat{p}(X)}.\\mu_1 (X) - \\frac{(1-D)}{1-\\hat{p}(X)}Y_{0} + \\frac{(1-D)}{1-\\hat{p}(X)}.\\mu_0 (X) \\right] = $$\n",
    "\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\frac{D}{\\hat{p}(X)}.Y_{1} - \\frac{(1-D)}{1-\\hat{p}(X)}Y_{0} + \\mu_1(X) - \\frac{D}{\\hat{p}(X)}.\\mu_1 (X) + \\frac{(1-D)}{1-\\hat{p}(X)}.\\mu_0 (X) - \\mu_0 (X) \\right] = $$\n",
    "\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\frac{D}{\\hat{p}(X)}.Y_{1} - \\frac{(1-D)}{1-\\hat{p}(X)}Y_{0} + \\mu_1(X) (1 - \\frac{D}{\\hat{p}(X)}) + \\mu_0 (X)(\\frac{(1-D)}{1-\\hat{p}(X)} - 1) \\right] = $$\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\frac{D}{\\hat{p}(X)}.Y_{1} - \\frac{(1-D)}{1-\\hat{p}(X)}Y_{0} + \\mu_1(X) (\\frac{\\hat{p}(X) - D}{\\hat{p}(X)}) + \\mu_0 (X)(\\frac{(1-D)- (1-\\hat{p}(X))}{1-\\hat{p}(X)}) \\right] = $$\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\frac{D}{\\hat{p}(X)}.Y_{1} - \\frac{(1-D)}{1-\\hat{p}(X)}Y_{0} \\right] = $$\n",
    "\n",
    "$$ \\mathbb{E} \\left[ Y^{1} - Y^{0} \\right] = $$\n",
    "\n",
    "$$ = \\beta $$\n",
    "\n",
    "A intuição é que, se $\\hat{p}$ está especificado corretamente, o $\\hat{\\beta}^{DR}$ é imparcial e o fator de ajuste desaparece, uma vez que os resíduos ($D_{i}-\\hat{p}(X)$) convergem para zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação em Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, a ideia é mostrar todos os componentes da Estimação Duplamente Robusta, e realizar as estimações na \"mão\" e, posteriormente, com o pacote pyDRReg.\n",
    "\n",
    "Primeiro precisamos estimar os componentes da Estimação Duplamente Robusta, \"escore de propensão\" ($p(X)$) e as funções de resposta ($\\mu_{0}(X)$ e $\\mu_{1}(X)$). Num primeiro momento vamos estimar o Propensity Score e estimar regressões ATE e ATE para o método de Propensity Score Weighting (PSW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando o escore de propensão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.446546\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                Treated   No. Observations:                 4642\n",
      "Model:                          Logit   Df Residuals:                     4638\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Thu, 05 Sep 2024   Pseudo R-squ.:                 0.07078\n",
      "Time:                        16:28:30   Log-Likelihood:                -2072.9\n",
      "converged:                       True   LL-Null:                       -2230.7\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.853e-68\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5894      0.230      2.564      0.010       0.139       1.040\n",
      "casada        -1.0310      0.090    -11.497      0.000      -1.207      -0.855\n",
      "mage           0.0122      0.008      1.570      0.116      -0.003       0.027\n",
      "medu          -0.1415      0.017     -8.540      0.000      -0.174      -0.109\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Regressão Logística para estimar o escore de propensão\n",
    "logit_model = smf.logit(\"Treated ~ 1 + casada + mage + medu\", data=df).fit()\n",
    "# Imprimindo o modelo\n",
    "print(logit_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o escore de propensão no DataFrame\n",
    "df['ps'] = logit_model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verificar graficamente a área de sobreposição\n",
    "sns.histplot(data=df, x='ps', hue='Treated', bins=100, stat='density', common_norm=False).\\\n",
    "    set(ylabel=\"\", title=\"Distribution of Propensity Scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos calcular os pesos IPW para o ATE e ATT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Probability of Treatment Weight (IPTW)\n",
    "\n",
    "# Peso para o efeito médio do tratamento (ATE)\n",
    "df['W1'] = 1 / df['ps']\n",
    "df.loc[df['Treated'] == 0, 'W1'] = 0\n",
    "df['W2'] = 1 / (1 - df['ps'])\n",
    "df.loc[df['Treated'] == 1, 'W2'] = 0\n",
    "\n",
    "# Peso para o efeito médio do tratamento nos tratados (ATT)\n",
    "df['W_ATE'] = df['W1'] + df['W2']\n",
    "df['W_ATT'] = df['ps'] / (1 - df['ps'])\n",
    "df.loc[df['Treated'] == 1, 'W_ATT'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A título de curiosidade, podemos estimar o efeito médio do tratamento para os tratados (ATT) e o efeito médio do tratamento (ATE) utilizando o método de Ponderação pelo Escore de Propensão (IPW). Para isso, basta rodar a regressão linear considerando como peso amostral os valores de $W_{ATE}$ e $W_{ATT}$, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.040\n",
      "Model:                            WLS   Adj. R-squared:                  0.040\n",
      "Method:                 Least Squares   F-statistic:                     194.9\n",
      "Date:                Thu, 05 Sep 2024   Prob (F-statistic):           2.03e-43\n",
      "Time:                        21:04:19   Log-Likelihood:                -36594.\n",
      "No. Observations:                4642   AIC:                         7.319e+04\n",
      "Df Residuals:                    4640   BIC:                         7.321e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3401.1645     11.754    289.358      0.000    3378.121    3424.208\n",
      "Treated     -234.4371     16.793    -13.960      0.000    -267.360    -201.515\n",
      "==============================================================================\n",
      "Omnibus:                     1345.722   Durbin-Watson:                   2.003\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            13048.433\n",
      "Skew:                          -1.100   Prob(JB):                         0.00\n",
      "Kurtosis:                      10.914   Cond. No.                         2.60\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Propensity Score Weighting - ATE\n",
    "psw_ate = smf.wls(\"Y ~ Treated\", weights=df['W_ATE'], data=df).fit()\n",
    "print(psw_ate.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.032\n",
      "Model:                            WLS   Adj. R-squared:                  0.032\n",
      "Method:                 Least Squares   F-statistic:                     155.8\n",
      "Date:                Thu, 05 Sep 2024   Prob (F-statistic):           3.41e-35\n",
      "Time:                        16:28:39   Log-Likelihood:                -37060.\n",
      "No. Observations:                4642   AIC:                         7.412e+04\n",
      "Df Residuals:                    4640   BIC:                         7.414e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3350.9689     12.014    278.924      0.000    3327.416    3374.522\n",
      "Treated     -213.3092     17.089    -12.482      0.000    -246.812    -179.807\n",
      "==============================================================================\n",
      "Omnibus:                     1802.084   Durbin-Watson:                   2.029\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            22353.766\n",
      "Skew:                          -1.499   Prob(JB):                         0.00\n",
      "Kurtosis:                      13.324   Cond. No.                         2.60\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#Propensity Score Weighting - ATT\n",
    "psw_att = smf.wls(\"Y ~ Treated\", weights=df['W_ATT'], data=df).fit()\n",
    "print(psw_att.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repare que podemos obter os mesmos resultados utilizando o pacote pyDRReg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator         IPW\n",
      "1        Method         ATT\n",
      "2      Estimate -213.309168\n",
      "3  bootstrap_SE   22.839909\n",
      "4        t-stat   -9.339318\n",
      "5       p-value         0.0\n",
      "6      CI Lower -258.075389\n",
      "7      CI Upper -168.542946\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "IPW_att = pyDRReg(df, X_vars, T_var, Y_var, method='att', estimator='IPW', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(IPW_att.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator         IPW\n",
      "1        Method         ATE\n",
      "2      Estimate -234.437125\n",
      "3  bootstrap_SE   23.479172\n",
      "4        t-stat   -9.984897\n",
      "5       p-value         0.0\n",
      "6      CI Lower -280.456303\n",
      "7      CI Upper -188.417947\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "IPW_ate = pyDRReg(df, X_vars, T_var, Y_var, method='ate', estimator='IPW', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(IPW_ate.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos calcular as funções de resposta \"$\\mu$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_mu(df, X, D, y, model_mu):\n",
    "    mu = model_mu.fit(df[X + [D]], df[y])\n",
    "    mu0 = mu.predict(df[X + [D]].assign(Treated=0))\n",
    "    mu1 = mu.predict(df[X + [D]].assign(Treated=1))\n",
    "    return mu0, mu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos o resultado da regressão linear com o estimador OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-219.32345600264168\n"
     ]
    }
   ],
   "source": [
    "X = ['casada', 'mage', 'medu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "\n",
    "mu0, mu1 = estimate_mu(df, X, D, y, LinearRegression())\n",
    "ols = np.mean(mu1 - mu0)\n",
    "print(ols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é o mesmo se utilizarmos a regressão linear com o smf do pacote statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.055\n",
      "Model:                            WLS   Adj. R-squared:                  0.054\n",
      "Method:                 Least Squares   F-statistic:                     67.86\n",
      "Date:                Thu, 05 Sep 2024   Prob (F-statistic):           6.80e-56\n",
      "Time:                        16:23:44   Log-Likelihood:                -35982.\n",
      "No. Observations:                4642   AIC:                         7.197e+04\n",
      "Df Residuals:                    4637   BIC:                         7.201e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   3155.5036     51.413     61.376      0.000    3054.710    3256.297\n",
      "Treated     -219.3235     22.031     -9.955      0.000    -262.515    -176.132\n",
      "casada       160.0091     20.939      7.642      0.000     118.959     201.059\n",
      "mage           2.1894      1.722      1.271      0.204      -1.187       5.565\n",
      "medu           6.0689      3.705      1.638      0.102      -1.195      13.333\n",
      "==============================================================================\n",
      "Omnibus:                      724.683   Durbin-Watson:                   2.000\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2256.187\n",
      "Skew:                          -0.802   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.015   Cond. No.                         188.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols2 = smf.wls(\"Y ~ Treated + casada + mage + medu\", data=df).fit()\n",
    "print(ols2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eu chamo atenção novamente que **o resultado do OLS é a diferença entre as funções de resposta. Isso será importante na abordagem futura de machine learning**. Agora temos tudo que precisamos para estimar o efeito médio do tratamento (ATE) e o efeito médio do tratamento para os tratados (ATT) utilizando a abordagem de Estimação Duplamente Robusta (DR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimativa Duplamente Robusta**\n",
    "\n",
    "Agora temos todos os componentes para estimar o DR para ATE e o ATT, vamos fazer \"na mão\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-229.19546059542853\n"
     ]
    }
   ],
   "source": [
    "# DR-ATE\n",
    "DR_ATE = mu1 - mu0 + df[\"Treated\"] / df[\"ps\"] * (df[\"Y\"] - mu1) - (1-df[\"Treated\"]) / (1-df[\"ps\"]) * (df[\"Y\"] - mu0)\n",
    "print(np.mean(DR_ATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-218.94795582708\n"
     ]
    }
   ],
   "source": [
    "# DR-ATT\n",
    "DR_ATT = mu1 - mu0 + df[\"Treated\"] * (df[\"Y\"] - mu1) - (1-df[\"Treated\"])*df[\"ps\"] / (1-df[\"ps\"]) * (df[\"Y\"] - mu0)\n",
    "print(np.mean(DR_ATT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o pacote pyDRReg para obter os mesmos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator          DR\n",
      "1        Method         ATE\n",
      "2      Estimate -229.195464\n",
      "3  bootstrap_SE   23.584023\n",
      "4        t-stat   -9.718251\n",
      "5       p-value         0.0\n",
      "6      CI Lower  -275.42015\n",
      "7      CI Upper -182.970779\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "DR_ate = pyDRReg(df, X_vars, T_var, Y_var, method='ate', estimator='DR', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(DR_ate.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Metric       Value\n",
      "0     Estimator          DR\n",
      "1        Method         ATT\n",
      "2      Estimate -218.947956\n",
      "3  bootstrap_SE   22.000454\n",
      "4        t-stat   -9.951974\n",
      "5       p-value         0.0\n",
      "6      CI Lower -262.068846\n",
      "7      CI Upper -175.827066\n"
     ]
    }
   ],
   "source": [
    "from pyDRReg.pyDRReg import pyDRReg\n",
    "\n",
    "T_var = 'Treated'\n",
    "Y_var = 'Y'\n",
    "X_vars = ['casada', 'mage', 'medu']\n",
    "\n",
    "DR_att = pyDRReg(df, X_vars, T_var, Y_var, method='att', estimator='DR', n_bootstrap=50, seed=44)\n",
    "\n",
    "print(DR_att.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse tipo de estimador é bastante importante na literatura. E já possui alguns estimadores que realizam as estimações de forma direta. Por exemplo, poderíamos computar diretamente com 'LinearDRLearner' da biblioteca 'EconML' da Microsoft (EconML - Estimate causal effects with ML).\n",
    "\n",
    "obs: https://www.microsoft.com/en-us/research/project/econml/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from econml.dr import LinearDRLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['casada', 'mage', 'medu']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearDRLearner(model_propensity=LogisticRegression(), \n",
    "                        model_regression=LinearRegression(),\n",
    "                        random_state=1)\n",
    "model.fit(Y=df[\"Y\"], T=df[\"Treated\"], X=X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Uncertainty of Mean Point Estimate</caption>\n",
       "<tr>\n",
       "  <th>mean_point</th> <th>stderr_mean</th>  <th>zstat</th> <th>pvalue</th> <th>ci_mean_lower</th> <th>ci_mean_upper</th>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>-228.121</td>    <td>23.167</td>    <td>-9.847</td>   <td>0.0</td>    <td>-273.527</td>      <td>-182.714</td>   \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.ate_inference(X=X.values, T0=0, T1=1).summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo nos dá diretamente o efeito médio do tratamento. A estimativa é estatisticamente diferente de zero e o intervalo de confiança inclui o valor verdadeiro de -229,17. Observe que obtivemos uma estimativa diferente porque a função **LinearDRLearner** também realizou o cross-fitting em segundo plano, o que não fizemos antes. Ele não calcula o ATT.\n",
    "\n",
    "Outro pacote importante é o \"causalml\" (https://causalml.readthedocs.io/en/latest/about.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boas práticas\n",
    "\n",
    "* Verifique o balanço das covariáveis.\n",
    "  * Tanto o IPW quanto o DR (AIPW) foram desenvolvidos para ambientes nos quais o tratamento não é atribuído aleatoriamente incondicionalmente, mas pode depender de algumas variáveis observáveis. Essas informações podem ser verificadas de duas maneiras: \n",
    "    * (1) Produza uma tabela de médias/equilíbrio das covariáveis. Se a randomização incondicional não for válida, esperamos ver diferenças significativas entre alguns observáveis; \n",
    "    * (2) Trace os escores de propensão estimados. Se a randomização incondicional for válida, esperamos que os escores de propensão sejam constantes.\n",
    "* Verifique a suposição de sobreposição.\n",
    "  * Podemos simplesmente verificar os limites dos escores de propensão previstos. Se a suposição de sobreposição for violada, acabamos dividindo algum termo do estimador por zero.\n",
    "* Usar o *cross-fitting* (ajuste cruzado quando estiver avaliando por machine learning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
